{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "model=\"openai/gpt-4o-mini\",\n",
    "openai_api_base=\"https://mlapi.run/40cc17ae-a89b-4f12-a7d6-13293180fc87/v1\",\n",
    "openai_api_key=\"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpYXQiOjE3NTcwNDA4ODAsIm5iZiI6MTc1NzA0MDg4MCwiZXhwIjoxNzU5MjQ0Mzk5LCJrZXlfaWQiOiI0ZGFjZTZlNi04ZjgyLTQ1ZDMtYTlkOC0wOGViMmYyZDM2YzcifQ.-pGdgMXkEwLGKeKnRt2LpE8gXexOPL0dLnqwrg4YXDA\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "OpenAIError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mChatOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-4o\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/langchain_core/load/serializable.py:111\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m\n\u001b[0;32m--> 111\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/langchain_openai/chat_models/base.py:516\u001b[0m, in \u001b[0;36mBaseChatOpenAI.validate_environment\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhttp_client \u001b[38;5;241m=\u001b[39m httpx\u001b[38;5;241m.\u001b[39mClient(proxy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopenai_proxy)\n\u001b[1;32m    515\u001b[0m     sync_specific \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp_client\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhttp_client}\n\u001b[0;32m--> 516\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_client \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mclient_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msync_specific\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    517\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39masync_client:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/openai/_client.py:105\u001b[0m, in \u001b[0;36mOpenAI.__init__\u001b[0;34m(self, api_key, organization, project, base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[0m\n\u001b[1;32m    103\u001b[0m     api_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[1;32m    106\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    107\u001b[0m     )\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m api_key\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m organization \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mOpenAIError\u001b[0m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 8, 'total_tokens': 17, 'completion_tokens_details': {'reasoning_tokens': 0, 'audio_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'cached_tokens': 0, 'audio_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_8bda4d3a2c', 'finish_reason': 'stop', 'logprobs': None}, id='run-9234ff3c-5ccf-428a-98f2-7ae24bb5c6e7-0', usage_metadata={'input_tokens': 8, 'output_tokens': 9, 'total_tokens': 17})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = llm.invoke(\"HI\")\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 간단한 형태의 챗봇 구현\n",
    "\n",
    "### 1. Output Parser 알아보기\n",
    "\n",
    "- LLM이 생성한 텍스트를 정형화된 형태로 추출하는 법을 배웁니다.\n",
    "\n",
    "### 2. Memory 추가 하기\n",
    "\n",
    "- 단발성이 아닌, 과거 히스토리를 기억한 상태에서 텍스트를 생성하는 법을 배웁니다.\n",
    "\n",
    "### 3. Chat message 깊이 파보기\n",
    "\n",
    "- Memory 기능을 가능하게 하는 LangChain의 주요 기능을 깊이 살펴봅니다.\n",
    "\n",
    "### 4. 간단한 챗봇 구현\n",
    "\n",
    "- 지금까지 배운 prompt, llm, 그리고 memory를 모두 결합하여 간단한 챗봇을 만듭니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Output Parser 알아보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"반지의 제왕\"에는 다양한 등장인물이 있습니다. 주요 인물 몇 명을 소개하자면:\\n\\n1. **프로도 밧긴스(Frodo Baggins)**: 이야기의 주인공으로, 반지를 파괴하기 위해 모험을 떠나는 호빗입니다.\\n2. **샘와이즈 감지(Samwise Gamgee)**: 프로도의 충실한 친구이자 정원사로, 프로도를 돕기 위해 함께 여행합니다.\\n3. **간달프(Gandalf)**: 강력한 마법사로, 반지의 위험을 인식하고 프로도와 그의 친구들을 돕는 역할을 합니다.\\n4. **아라곤(Aragorn)**: 왕가의 후손이며, 프로도의 여정에서 중요한 지도자로 등장합니다.\\n5. **레골라스(Legolas)**: 엘프 전사로, 아라곤과 함께 프로도 일행의 일원입니다.\\n6. **김리(Gimli)**: 드워프 전사로, 레골라스와 함께 협력하며 우정의 상징적인 관계를 맺습니다.\\n7. **갈라드리엘(Galadriel)**: 강력한 엘프 여왕으로, 프로도와 그의 일행에게 도움을 줍니다.\\n8. **사우론(Sauron)**: 이야기의 주 antagonist로, 반지를 되찾으려는 악의 세력의 수장입니다.\\n\\n이 외에도 많은 캐릭터가 등장하지만, 위 인물들이 주요한 역할을 맡고 있습니다. 더 궁금한 인물이 있다면 말씀해 주세요!'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# template를 정의\n",
    "template = \"{movie_name}에 나오는 등장인물을 알려줘.\"\n",
    "\n",
    "# from_template : Langchain의 promptTemplate 객체를 생성\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# LCEL을 활용하여 chain 객체 생성\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "chain.invoke({\"movie_name\":\"반지의 제왕\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='\"반지의 제왕\"에 등장하는 주요 인물들은 다음과 같습니다:\\n\\n1. **프로도 백엔드 (Frodo Baggins)** - 이야기를 이끄는 주인공으로, 반지를 파괴하기 위해 모험을 떠나는 호빗입니다.\\n2. **샘와이즈 갬지 (Samwise Gamgee)** - 프로도의 충실한 친구이자 동행자로, 그의 모험을 돕습니다.\\n3. **간달프 (Gandalf)** - 강력한 마법사이자, 반지 원정대의 지도자 중 한 명입니다.\\n4. **아라곤 (Aragorn)** - 인간 왕국의 후계자이며, 반지 원정대의 일원으로 프로도를 보호합니다.\\n5. **레골라스 (Legolas)** - 엘프의 궁수로, 반지 원정대의 일원입니다.\\n6. **김리 (Gimli)** - 난쟁이 전사로, 레골라스와 친구가 됩니다.\\n7. **보로미르 (Boromir)** - 인간 전사로, 반지의 힘에 유혹당하지만 결국 희생합니다.\\n8. **골룸 (Gollum)** - 반지를 소유했던 과거가 있는 생명체로, 프로도와 샘을 따라다니며 반지를 되찾으려 합니다.\\n9. **사우론 (Sauron)** - 이야기의 악당으로, 반지를 통해 세계를 지배하려고 합니다.\\n10. **갈라드리엘 (Galadriel)** - 엘프의 여왕으로, 반지 원정대에게 도움을 줍니다.\\n\\n이 외에도 많은 인물들이 등장하며, 각자의 이야기와 배경이 있습니다.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 381, 'prompt_tokens': 21, 'total_tokens': 402, 'completion_tokens_details': {'reasoning_tokens': 0, 'audio_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'cached_tokens': 0, 'audio_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_8bda4d3a2c', 'finish_reason': 'stop', 'logprobs': None}, id='run-de5b9114-7247-45ef-8b1d-9769221b24c6-0', usage_metadata={'input_tokens': 21, 'output_tokens': 381, 'total_tokens': 402})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | llm\n",
    "\n",
    "chain.invoke({\"movie_name\":\"반지의 제왕\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.output_parsers import PydanticOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Food(BaseModel):\n",
    "    name: str = Field(description=\"name of a food\")\n",
    "    ingredients: List[str] = Field(description=\"list of names of the ingredients mentioned\")\n",
    "\n",
    "food_query = \"What are the ingredients used for making pizza?\"\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=Food)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import OutputFixingParser\n",
    "\n",
    "new_parser = OutputFixingParser.from_llm(parser=parser, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = llm | new_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = chain.invoke(food_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Memory 추가 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `save_context`를 활용하여 history를 추가할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_63/3471961393.py:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory()\n"
     ]
    }
   ],
   "source": [
    "memory = ConversationBufferMemory()\n",
    "memory.save_context(\n",
    "    inputs={\n",
    "        \"user\": \"내가 영화를 보고 싶은데 어떤걸 봐야 할지 모르겠어.\"\n",
    "    },\n",
    "    outputs={\n",
    "        \"assistant\": \"안녕하세요, 영화 추천이라고 하면 제가 전문이죠! 어떤 장르의 영화를 보고싶으신가요?\"\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConversationBufferMemory(chat_memory=InMemoryChatMessageHistory(messages=[HumanMessage(content='내가 영화를 보고 싶은데 어떤걸 봐야 할지 모르겠어.', additional_kwargs={}, response_metadata={}), AIMessage(content='안녕하세요, 영화 추천이라고 하면 제가 전문이죠! 어떤 장르의 영화를 보고싶으신가요?', additional_kwargs={}, response_metadata={})]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context(\n",
    "    inputs={\"human\": \"액션 영화를 보고싶어. 판타지가 조금 가미된.\"},\n",
    "    outputs={\n",
    "        \"ai\": \"네, 그렇다면 반지의 제왕을 추천합니다!\"\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: 내가 영화를 보고 싶은데 어떤걸 봐야 할지 모르겠어.\\nAI: 안녕하세요, 영화 추천이라고 하면 제가 전문이죠! 어떤 장르의 영화를 보고싶으신가요?\\nHuman: 액션 영화를 보고싶어. 판타지가 조금 가미된.\\nAI: 네, 그렇다면 반지의 제왕을 추천합니다!'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `clear`를 활용하여 현재까지의 'history'를 삭제할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# memory.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Chat message 깊이 파보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <span style=\"background-color: #A5B68D; border-radius: 5px; padding: 2px 6px; border: 1px solid #ccc; font-family: sans-serif;\">\n",
    "    HumanMessage</span> : 사용자 메세지를 담은 객체입니다.\n",
    "- <span style=\"background-color: #A5B68D; border-radius: 5px; padding: 2px 6px; border: 1px solid #ccc; font-family: sans-serif;\">\n",
    "    AIMessage</span> : LLM 메세지를 담은 객체입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='안녕하세요! 어떻게 도와드릴까요?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 10, 'total_tokens': 20, 'completion_tokens_details': {'reasoning_tokens': 0, 'audio_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'cached_tokens': 0, 'audio_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_8bda4d3a2c', 'finish_reason': 'stop', 'logprobs': None}, id='run-5599fdcb-f953-4ec5-abae-3b9ae3859b3d-0', usage_metadata={'input_tokens': 10, 'output_tokens': 10, 'total_tokens': 20})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "llm.invoke([HumanMessage(content=\"안녕?\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='좋아요! 영어 이름을 만드는 데 도움이 되도록 몇 가지 질문을 드릴게요.\\n\\n1. 어떤 느낌의 이름을 원하시나요? (예: 세련된, 귀여운, 강한 등)\\n2. 선호하는 이니셜이나 소리나는 부분이 있나요?\\n3. 특별히 좋아하는 이름이나 관련된 의미가 있나요?\\n\\n이 질문에 답해주시면, 그에 맞춰 이름을 제안해드릴게요!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 100, 'prompt_tokens': 36, 'total_tokens': 136, 'completion_tokens_details': {'reasoning_tokens': 0, 'audio_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'cached_tokens': 0, 'audio_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_8bda4d3a2c', 'finish_reason': 'stop', 'logprobs': None}, id='run-dfc6c442-7fb2-4f13-80fa-1c5fc3009acc-0', usage_metadata={'input_tokens': 36, 'output_tokens': 100, 'total_tokens': 136})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "llm.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"안녕?\"),\n",
    "        AIMessage(content=\"안녕하세요. 오늘은 어떻게 도와드릴까요?\"),\n",
    "        HumanMessage(content=\"내 영어 이름을 만들어줘\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"background-color: #A5B68D; border-radius: 5px; padding: 2px 6px; border: 1px solid #ccc; font-family: sans-serif;\">\n",
    "    Runnable</span>이란, langchain의 컴포넌트를 실행하고 연결할 수 있도록 하는 주요 객체입니다.\n",
    "\n",
    "이전 `component1 | component2 | component3` 와 같은 chain이 가능한 이유이기도 합니다.\n",
    "\n",
    "그 중 `RunnableWithMessageHistory`은 다른 runnable을 감싸어 chat message history를 읽고 업데이트 할 수 있도록 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.chat_history import (\n",
    "    BaseChatMessageHistory,\n",
    "    InMemoryChatMessageHistory,\n",
    ")\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "store = {}\n",
    "\n",
    "# session_id를 key로 삼아 dictionary에 저장\n",
    "# value 값에는 chat history를 저장\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "# get_session_history를 통해 우리가 관리하는 \n",
    "with_message_history = RunnableWithMessageHistory(llm, get_session_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Runnable에 메세지와 memory를 위한 configuration 값들을 제공합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'안녕, 엘리스! 만나서 반가워. 어떻게 도와줄 수 있을까?'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"movie123\"}}\n",
    "\n",
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"안녕, 내 이름은 엘리스야.\")],\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'movie123': InMemoryChatMessageHistory(messages=[HumanMessage(content='안녕, 내 이름은 엘리스야.', additional_kwargs={}, response_metadata={}), AIMessage(content='안녕, 엘리스! 만나서 반가워. 어떻게 도와줄 수 있을까?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 18, 'total_tokens': 39, 'completion_tokens_details': {'reasoning_tokens': 0, 'audio_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'cached_tokens': 0, 'audio_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_8bda4d3a2c', 'finish_reason': 'stop', 'logprobs': None}, id='run-9d14233d-537a-49c1-bf56-4898209527cf-0', usage_metadata={'input_tokens': 18, 'output_tokens': 21, 'total_tokens': 39})])}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'네, 당신의 이름은 엘리스라고 하셨어요. 맞나요?'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"movie123\"}}\n",
    "\n",
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"내 이름이 뭐라고 했는지 기억나?\")],\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'movie123': InMemoryChatMessageHistory(messages=[HumanMessage(content='안녕, 내 이름은 엘리스야.', additional_kwargs={}, response_metadata={}), AIMessage(content='안녕, 엘리스! 만나서 반가워. 어떻게 도와줄 수 있을까?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 18, 'total_tokens': 39, 'completion_tokens_details': {'reasoning_tokens': 0, 'audio_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'cached_tokens': 0, 'audio_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_8bda4d3a2c', 'finish_reason': 'stop', 'logprobs': None}, id='run-9d14233d-537a-49c1-bf56-4898209527cf-0', usage_metadata={'input_tokens': 18, 'output_tokens': 21, 'total_tokens': 39}), HumanMessage(content='내 이름이 뭐라고 했는지 기억나?', additional_kwargs={}, response_metadata={}), AIMessage(content='네, 당신의 이름은 엘리스라고 하셨어요. 맞나요?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 58, 'total_tokens': 76, 'completion_tokens_details': {'reasoning_tokens': 0, 'audio_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'cached_tokens': 0, 'audio_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_8bda4d3a2c', 'finish_reason': 'stop', 'logprobs': None}, id='run-f72968a3-f84e-4a02-b31e-ccce3eb6458d-0', usage_metadata={'input_tokens': 58, 'output_tokens': 18, 'total_tokens': 76})])}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "config 변경을 통해 상호작용할 memory를 변경할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'죄송하지만, 이전 대화 내용을 기억할 수 없어요. 만약 이름을 알려주시면 다시 말씀드릴 수 있습니다!'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"random123\"}}\n",
    "\n",
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"내 이름이 뭐라고 했는지 기억나?\")],\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'movie123': InMemoryChatMessageHistory(messages=[HumanMessage(content='안녕, 내 이름은 엘리스야.', additional_kwargs={}, response_metadata={}), AIMessage(content='안녕, 엘리스! 만나서 반가워. 어떻게 도와줄 수 있을까?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 18, 'total_tokens': 39, 'completion_tokens_details': {'reasoning_tokens': 0, 'audio_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'cached_tokens': 0, 'audio_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_8bda4d3a2c', 'finish_reason': 'stop', 'logprobs': None}, id='run-9d14233d-537a-49c1-bf56-4898209527cf-0', usage_metadata={'input_tokens': 18, 'output_tokens': 21, 'total_tokens': 39}), HumanMessage(content='내 이름이 뭐라고 했는지 기억나?', additional_kwargs={}, response_metadata={}), AIMessage(content='네, 당신의 이름은 엘리스라고 하셨어요. 맞나요?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 58, 'total_tokens': 76, 'completion_tokens_details': {'reasoning_tokens': 0, 'audio_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'cached_tokens': 0, 'audio_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_8bda4d3a2c', 'finish_reason': 'stop', 'logprobs': None}, id='run-f72968a3-f84e-4a02-b31e-ccce3eb6458d-0', usage_metadata={'input_tokens': 58, 'output_tokens': 18, 'total_tokens': 76})]),\n",
       " 'random123': InMemoryChatMessageHistory(messages=[HumanMessage(content='내 이름이 뭐라고 했는지 기억나?', additional_kwargs={}, response_metadata={}), AIMessage(content='죄송하지만, 이전 대화 내용을 기억할 수 없어요. 만약 이름을 알려주시면 다시 말씀드릴 수 있습니다!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 28, 'prompt_tokens': 18, 'total_tokens': 46, 'completion_tokens_details': {'reasoning_tokens': 0, 'audio_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'cached_tokens': 0, 'audio_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_8bda4d3a2c', 'finish_reason': 'stop', 'logprobs': None}, id='run-5669ee3c-9b84-4604-9424-55d2f4cfd366-0', usage_metadata={'input_tokens': 18, 'output_tokens': 28, 'total_tokens': 46})])}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 간단한 챗봇 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`MessagesPlaceholder`를 활용하여 prompt template에 message history가 들어갈 위치를 미리 지정할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\", \"당신은 {movie_name}에 관해서만 이야기를 하고 싶어하는 영화 전문가 입니다.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chat_chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = {}\n",
    "\n",
    "# session_id를 key로 삼아 dictionary에 저장함. value 값에는 chat history를 저장함\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chat_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "chat_chain 자체가 runnable이기 때문에 `RunnableWithMessageHistory`를 활용하여 memory를 연동한 runnable로 구성할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'죄송하지만, 반지의 제왕에 관해서만 이야기할 수 있습니다. 반지의 제왕에 대해 궁금한 점이나 특정 캐릭터, 이야기 줄거리 등에 대해 말씀해 주시면 기꺼이 도와드리겠습니다!'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"movie_chat\"}}\n",
    "\n",
    "response = with_message_history.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"안녕, 분노의 질주에 누가 나오는지 알려줄 수 있어?\")], \"movie_name\": \"반지의 제왕\"},\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "따라서 invoke를 할 때 'message'나 'movie_name'과 같은 변수를 input에 추가하여 chat_chain의 인풋으로 제공할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'반지의 제왕 영화 시리즈는 총 3부작으로 구성되어 있습니다. 각각의 영화는 J.R.R. 톨킨의 소설을 바탕으로 하고 있으며, 다음과 같은 제목을 가지고 있습니다:\\n\\n1. **반지의 제왕: 반지 원정대** (The Lord of the Rings: The Fellowship of the Ring, 2001)\\n2. **반지의 제왕: 두 개의 탑** (The Lord of the Rings: The Two Towers, 2002)\\n3. **반지의 제왕: 왕의 귀환** (The Lord of the Rings: The Return of the King, 2003)\\n\\n이 세 작품은 각각의 이야기를 이어가며, 중간계의 전쟁과 모험을 다루고 있습니다. 추가로 궁금한 점이 있다면 말씀해 주세요!'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "response = with_message_history.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"그렇다면 반지의 제왕은 총 몇 부작이야?\")], \"movie_name\": \"반지의 제왕\"},\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "<span style=\"color:rgb(120, 120, 120)\">본 학습 자료를 포함한 사이트 내 모든 자료의 저작권은 엘리스에 있으며 외부로의 무단 복제, 배포 및 전송을 불허합니다.\n",
    "\n",
    "Copyright @ elice all rights reserved</span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
