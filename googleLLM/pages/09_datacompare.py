import os
import sys
import io
import zipfile
from typing import Any, Dict, List, Optional

import pandas as pd
import streamlit as st
from dotenv import load_dotenv
import matplotlib.pyplot as plt
import duckdb

# LangChain ë° Gemini ë¼ì´ë¸ŒëŸ¬ë¦¬
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_community.callbacks.streamlit import StreamlitCallbackHandler
from langchain_community.chat_message_histories import StreamlitChatMessageHistory
from langchain_experimental.tools.python.tool import PythonAstREPLTool
from langchain.agents import create_react_agent, AgentExecutor
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.tools import tool
from langchain.callbacks import StdOutCallbackHandler
from langchain_core.callbacks.base import BaseCallbackHandler

# =======================================================================
# ğŸ› ï¸ ì½œë°± í´ë˜ìŠ¤ ìˆ˜ì •: BaseCallbackHandler ìƒì† (ì˜¤ë¥˜ í•´ê²°)
# =======================================================================
class SimpleCollectCallback(BaseCallbackHandler):
    """
    LangChain AgentExecutorì™€ì˜ í˜¸í™˜ì„±ì„ ìœ„í•´ BaseCallbackHandlerë¥¼ ìƒì†ë°›ìŠµë‹ˆë‹¤.
    (ì›ë˜ì˜ CollectAllCallback ëª¨ë“ˆ ëŒ€ì²´ìš©)
    """
    def __init__(self):
        self.events = []
    
    def on_tool_error(self, error: Exception, **kwargs: Any) -> Any:
        # AgentExecutorê°€ ì˜¤ë¥˜ ë°œìƒ ì‹œ í˜¸ì¶œí•˜ëŠ” í•„ìˆ˜ ë©”ì„œë“œ
        self.events.append({"type": "tool_error", "error": str(error)})
        
    def on_llm_start(self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any) -> None:
        self.events.append({"type": "llm_start", "prompts": prompts})
        
    def on_llm_end(self, response: Any, **kwargs: Any) -> None:
        self.events.append({"type": "llm_end"})

    # AgentExecutor í˜¸í™˜ì„± ìœ ì§€ë¥¼ ìœ„í•´ í•„ìˆ˜ì ì¸ ìµœì†Œ ë©”ì„œë“œ ì •ì˜
    def on_agent_action(self, action: Any, **kwargs: Any) -> Any:
        pass
    
    def on_agent_finish(self, finish: Any, **kwargs: Any) -> Any:
        pass

# =============================
# ğŸš€ App ì‹œì‘ ë° ì„¤ì •
# =============================
load_dotenv()

st.set_page_config(page_title="DF Chatbot (Gemini)", page_icon="âœ¨", layout="wide")
st.title("âœ¨ DataFrame Chatbot (Gemini + LangChain)")
st.caption("Gemini + Python tool(ReAct)ë¡œ DataFrameì„ ë¶„ì„í•˜ê³  ì´ìƒì ì„ ê²€í† í•©ë‹ˆë‹¤. + ë‘ CSV ë¹„êµ")

# =============================
# ğŸ“ ë°ì´í„° ë¡œë“œ ë° í™˜ê²½ ë³€ìˆ˜
# =============================
# âš ï¸ DATA_DIR ì´ˆê¸°í™”
DEFAULT_DATA_DIR = os.getenv("DATA_DIR", "/Users/najongseong/dataset")
DFB_DEFAULT_NAME = "telemetry_raw.csv" # df_B ê¸°ë³¸ íŒŒì¼ëª…

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” ë° DATA_DIR ì„¤ì •
if "DATA_DIR" not in st.session_state:
    st.session_state["DATA_DIR"] = DEFAULT_DATA_DIR
if "df_A_data" not in st.session_state:
    st.session_state["df_A_data"] = None
if "df_A_name" not in st.session_state:
    st.session_state["df_A_name"] = "No Data"
if "csv_path" not in st.session_state:
    # ì´ˆê¸° csv_pathëŠ” ê¸°ë³¸ ë””ë ‰í† ë¦¬ì˜ ê¸°ë³¸ íŒŒì¼ë¡œ ì„¤ì • (ë¡œë“œê°€ ë  ê²½ìš°)
    st.session_state["csv_path"] = os.path.join(DEFAULT_DATA_DIR, "stormtrooper.csv") 

# âœ… df_B ê´€ë ¨ ì„¸ì…˜ ìƒíƒœ
if "df_B_data" not in st.session_state:
    st.session_state["df_B_data"] = None
if "df_B_name" not in st.session_state:
    st.session_state["df_B_name"] = "No Data"
if "csv_b_path" not in st.session_state:
    st.session_state["csv_b_path"] = ""

# í˜„ì¬ ì‚¬ìš© DATA_DIR
DATA_DIR = st.session_state["DATA_DIR"]
DFB_DEFAULT = os.path.join(DATA_DIR, DFB_DEFAULT_NAME)

# -----------------------------------------------------------------------
# ğŸ”„ ë°ì´í„° íŒŒì¼ ë¡œë” ê³µí†µ ìœ í‹¸ ë° ì„ íƒ ë¡œì§
# -----------------------------------------------------------------------

SUPPORTED_EXTENSIONS = (".csv", ".parquet")

def _read_table(path: str) -> pd.DataFrame:
    """í™•ì¥ìì— ë§ê²Œ CSV ë˜ëŠ” Parquetë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    ext = os.path.splitext(path)[1].lower()
    if ext == ".csv":
        return pd.read_csv(path)
    if ext == ".parquet":
        return pd.read_parquet(path)
    raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤: {ext}")

def load_df_A(path: str, display_name: str):
    """ì§€ì •ëœ ê²½ë¡œì—ì„œ df_Aë¥¼ ë¡œë“œí•˜ê³  ì„¸ì…˜ ìƒíƒœë¥¼ ì—…ë°ì´íŠ¸í•˜ëŠ” í—¬í¼ í•¨ìˆ˜"""
    try:
        new_df = _read_table(path)
        st.session_state["df_A_data"] = new_df
        st.session_state["df_A_name"] = display_name
        # âœ… ì„ íƒí•œ íŒŒì¼ì˜ ì „ì²´ ê²½ë¡œë¥¼ csv_pathì— ì •í™•íˆ ì—°ê²°
        st.session_state["csv_path"] = path 
        return True, f"Loaded file: {display_name} (Shape: {new_df.shape})"
    except Exception as e:
        st.session_state["df_A_data"] = None
        st.session_state["df_A_name"] = "Load Failed"
        # ë¡œë“œ ì‹¤íŒ¨ ì‹œ ê²½ë¡œë„ ì´ˆê¸°í™”
        st.session_state["csv_path"] = "" 
        return False, f"ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {path}\n{e}"

def load_df_B(path: str, display_name: str):
    """ì§€ì •ëœ ê²½ë¡œì—ì„œ df_Bë¥¼ ë¡œë“œí•˜ê³  ì„¸ì…˜ ìƒíƒœë¥¼ ì—…ë°ì´íŠ¸í•˜ëŠ” í—¬í¼ í•¨ìˆ˜"""
    try:
        new_df = _read_table(path)
        st.session_state["df_B_data"] = new_df
        st.session_state["df_B_name"] = display_name
        st.session_state["csv_b_path"] = path 
        return True, f"Loaded file (df_B): {display_name} (Shape: {new_df.shape})"
    except Exception as e:
        st.session_state["df_B_data"] = None
        st.session_state["df_B_name"] = "Load Failed"
        st.session_state["csv_b_path"] = "" 
        return False, f"df_B ë¡œë“œ ì‹¤íŒ¨: {path}\n{e}"

with st.sidebar:
    st.markdown("### ğŸ—‚ï¸ 1. ë°ì´í„° í´ë” ì„¤ì •")
    
    # DATA_DIR ì…ë ¥ í•„ë“œ
    new_data_dir = st.text_input(
        "Enter Data Directory Path",
        value=st.session_state["DATA_DIR"],
        key="data_dir_input"
    )

    if st.button("Set Directory"):
        if os.path.isdir(new_data_dir):
            st.session_state["DATA_DIR"] = new_data_dir
            st.session_state["df_A_data"] = None # í´ë” ë³€ê²½ ì‹œ ë°ì´í„° ì´ˆê¸°í™”
            st.session_state["df_A_name"] = "No Data"
            st.session_state["csv_path"] = "" 
            # âœ… df_Bë„ ì´ˆê¸°í™”
            st.session_state["df_B_data"] = None
            st.session_state["df_B_name"] = "No Data"
            st.session_state["csv_b_path"] = "" 
            st.success(f"Directory set to: `{new_data_dir}`")
            # ë””ë ‰í† ë¦¬ ë³€ê²½ í›„ ì¬ì‹¤í–‰í•˜ì—¬ íŒŒì¼ ëª©ë¡ì„ ì—…ë°ì´íŠ¸
            st.rerun() 
        else:
            st.error(f"Invalid directory path: `{new_data_dir}`")

    # ê°±ì‹ ëœ DATA_DIR
    DATA_DIR = st.session_state["DATA_DIR"]
    DFB_DEFAULT = os.path.join(DATA_DIR, DFB_DEFAULT_NAME)

    st.markdown("---")
    st.markdown("### ğŸ“„ 2. df_A ë°ì´í„° íŒŒì¼ ì„ íƒ")
    st.caption(f"Search directory: `{DATA_DIR}`")
    
    # DATA_DIRì—ì„œ ì§€ì› íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    data_files = []
    try:
        if os.path.isdir(DATA_DIR):
            for f in os.listdir(DATA_DIR):
                if f.lower().endswith(SUPPORTED_EXTENSIONS):
                    data_files.append(f)
            data_files.sort()
        else:
            st.warning("ìœ íš¨í•œ ë°ì´í„° ë””ë ‰í† ë¦¬ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
    except Exception as e:
        st.error(f"í´ë” ì ‘ê·¼ ì˜¤ë¥˜: {e}")

    # íŒŒì¼ ì„ íƒ SelectBox
    selected_file = st.selectbox(
        "Select data file for df_A",
        options=["--- Select a file ---"] + data_files,
        key="file_selector"
    )
    
    # íŒŒì¼ ë¡œë“œ ë²„íŠ¼
    if st.button("Load Selected File (df_A)"):
        if selected_file and selected_file != "--- Select a file ---":
            file_path = os.path.join(DATA_DIR, selected_file)
            success, message = load_df_A(file_path, selected_file) 
            if success:
                st.success(message)
                st.rerun() # íŒŒì¼ ë¡œë“œ í›„ UI ì—…ë°ì´íŠ¸ë¥¼ ìœ„í•´ ì¬ì‹¤í–‰
            else:
                st.error(message)
        else:
            st.warning("íŒŒì¼ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")

    st.markdown("---")
    st.markdown("### ğŸ“„ 3. df_B ë°ì´í„° íŒŒì¼ ì„ íƒ(ë¹„êµìš©)")
    selected_file_b = st.selectbox(
        "Select data file for df_B",
        options=["--- Select a file ---"] + data_files,
        key="file_selector_b"
    )

    if st.button("Load Selected File (df_B)"):
        if selected_file_b and selected_file_b != "--- Select a file ---":
            file_path_b = os.path.join(DATA_DIR, selected_file_b)
            success, message = load_df_B(file_path_b, selected_file_b)
            if success:
                st.success(message)
                st.rerun()
            else:
                st.error(message)
        else:
            st.warning("df_B íŒŒì¼ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")

    st.markdown("---")
    # í˜„ì¬ ë¡œë“œëœ íŒŒì¼ ê²½ë¡œ í‘œì‹œ (ë””ë²„ê¹…ìš©)
    st.caption(f"**í˜„ì¬ ë¡œë“œ íŒŒì¼ ê²½ë¡œ(df_A):** `{st.session_state.get('csv_path', 'Not loaded')}`")
    st.caption(f"**í˜„ì¬ ë¡œë“œ íŒŒì¼ ê²½ë¡œ(df_B):** `{st.session_state.get('csv_b_path', 'Not loaded')}`")
    st.caption(f"df_B ê¸°ë³¸ ê°€ì • íŒŒì¼: `{os.path.basename(DFB_DEFAULT)}`")

# ìµœì¢… df ê²°ì •
df = st.session_state["df_A_data"]
df_b = st.session_state["df_B_data"]
DATA_PATH_DISPLAY = st.session_state["df_A_name"]

if df is None:
    st.error("ë¶„ì„í•  DataFrame (df_A)ì„ ë¡œë“œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ìœ íš¨í•œ ë””ë ‰í† ë¦¬ì™€ ì§€ì›ë˜ëŠ” ë°ì´í„° íŒŒì¼ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")
    st.stop()

# =============================
# ğŸ–¼ï¸ Preview
# =============================
st.subheader("Preview")
st.write(f"**Loaded file for df_A:** `{DATA_PATH_DISPLAY}` (Shape: {df.shape})")
st.dataframe(df.head(10), use_container_width=True)

# df_B í”„ë¦¬ë·°(ì„ íƒ ì‹œ)
if df_b is not None:
    with st.expander(f"df_B Preview â€” {st.session_state['df_B_name']} (Shape: {df_b.shape})", expanded=False):
        st.dataframe(df_b.head(10), use_container_width=True)

# =============================
# ğŸ’¬ Chat history
# =============================
history = StreamlitChatMessageHistory(key="lc_msgs:dfchat")

# =============================
# ğŸ› ï¸ Tools ì •ì˜ ë° Agent Globals (DATA_DIR ë° df ë³€ìˆ˜ ì—…ë°ì´íŠ¸)
# =============================

# 1) Python ì‹¤í–‰ íˆ´: df_A/df_B/df/loading_df ë…¸ì¶œ
pytool = PythonAstREPLTool(
    globals={
        "pd": pd,
        "plt": plt,
        "df": df,      # í˜¸í™˜ì„± ìœ ì§€ (df == df_A)
        "df_A": df,    # ë©”ì¸ ë°ì´í„°ì…‹
        "df_B": df_b,  # ë³´ì¡° ë°ì´í„°ì…‹ (UI ë˜ëŠ” íˆ´ë¡œ ë¡œë“œ)
        "df_join": None,
        "loading_df": None,  # ad-hoc ë¶„ì„ìš©
        "duckdb": duckdb,
    },
    name="python_repl_ast",
    description=(
        "Execute Python on df_A/df_B with pandas/matplotlib. 'df' aliases df_A. "
        "Use this for custom computation, plotting (e.g., boxplot for outliers), and advanced data manipulation."
    )
)

# 2) ë™ì  íŒŒì¼ ë¡œë“œ (loading_df)
@tool
def load_loading_csv(filename: str) -> str:
    """Load a CSV or Parquet from DATA_DIR into 'loading_df'. Pass only file name (e.g., 'sample.csv' or 'sample.parquet')."""
    # íˆ´ ì‹¤í–‰ ì‹œ í˜„ì¬ DATA_DIRì„ ì‚¬ìš©
    current_data_dir = st.session_state.get("DATA_DIR", DEFAULT_DATA_DIR)
    path = os.path.join(current_data_dir, filename)
    try:
        new_df = _read_table(path)
    except Exception as e:
        return f"Failed to load {path}: {e}"
    pytool.globals["loading_df"] = new_df
    preview = new_df.head(10).to_markdown(index=False)
    shape = f"{new_df.shape[0]} rows x {new_df.shape[1]} cols"
    return f"Loaded {filename} from {current_data_dir} into loading_df\nShape: {shape}\n\nPreview (head):\n{preview}"

# 3) ì»¬ëŸ¼ ìš”ì•½ (loading_df ìš°ì„  â†’ ì—†ìœ¼ë©´ df_A)
@tool
def describe_columns(cols: str = "") -> str:
    """
    Describe selected columns (comma-separated).
    Uses 'loading_df' if available; otherwise uses 'df_A'.
    This is useful for initial data quality checks and outlier boundary overview.
    """
    if "loading_df" in pytool.globals and pytool.globals["loading_df"] is not None:
        current_df = pytool.globals["loading_df"]
        source = "loading_df"
    else:
        current_df = pytool.globals.get("df_A", df) if pytool.globals.get("df_A") is not None else df
        source = "df_A"

    use_cols = [c.strip() for c in cols.split(",") if c.strip()] or list(current_df.columns)
    missing = [c for c in use_cols if c not in current_df.columns]
    if missing:
        return f"Missing columns: {missing}\nAvailable: {list(current_df.columns)}"

    desc = current_df[use_cols].describe(include="all").transpose()
    shape = f"{current_df.shape[0]} rows x {current_df.shape[1]} cols"
    return f"[source={source} | shape={shape}]\n\n" + desc.to_markdown()

# 4) í”Œë¡¯ ZIP
@tool
def save_plots_zip() -> str:
    """Zip all current matplotlib figures. Use after plotting with python_repl_ast."""
    figs = [plt.figure(n) for n in plt.get_fignums()]
    if not figs:
        return "No figures to save."
    buf = io.BytesIO()
    with zipfile.ZipFile(buf, "w", zipfile.ZIP_DEFLATED) as zf:
        for i, f in enumerate(figs, 1):
            img = io.BytesIO()
            f.savefig(img, format="png", dpi=100, bbox_inches="tight")
            img.seek(0)
            zf.writestr(f"plot_{i}.png", img.read())
    return f"Zipped {len(figs)} plots. Bytes={len(buf.getvalue())}"

# 5) df_B ë¡œë”
@tool
def load_df_b(filename: str = "") -> str:
    """
    Load a CSV or Parquet into 'df_B'. If 'filename' is empty, defaults to 'telemetry_raw.csv' under DATA_DIR.
    Example: load_df_b()
    """
    current_data_dir = st.session_state.get("DATA_DIR", DEFAULT_DATA_DIR)
    
    if not filename:
        path = os.path.join(current_data_dir, DFB_DEFAULT_NAME)
        show_name = os.path.basename(path)
    else:
        path = os.path.join(current_data_dir, filename) if not os.path.isabs(filename) else filename
        show_name = os.path.basename(path)

    try:
        new_df = _read_table(path)
    except Exception as e:
        return f"Failed to load df_B from {path}: {e}"
    pytool.globals["df_B"] = new_df
    preview = new_df.head(10).to_markdown(index=False)
    return f"Loaded df_B from '{show_name}' (full: {path}) with shape {new_df.shape}\n\nPreview:\n{preview}"

# 6) df_A / df_B ëŒ€ìƒ ì§€ì • ìš”ì•½
@tool
def describe_columns_on(target: str = "A", cols: str = "") -> str:
    """
    Describe columns from df_A or df_B.
    target: 'A' or 'B'
    cols: comma-separated; empty -> all
    """
    t = (target or "A").strip().upper()
    if t == "B":
        if "df_B" not in pytool.globals or pytool.globals["df_B"] is None:
            return "df_B is not loaded. Use load_df_b() first."
        current_df = pytool.globals["df_B"]
        source = "df_B"
    else:
        current_df = pytool.globals.get("df_A", df) if pytool.globals.get("df_A") is not None else df
        source = "df_A"

    use_cols = [c.strip() for c in cols.split(",") if c.strip()] or list(current_df.columns)
    missing = [c for c in use_cols if c not in current_df.columns]
    if missing:
        return f"Missing columns: {missing}\nAvailable: {list(current_df.columns)}"

    desc = current_df[use_cols].describe(include="all").transpose()
    shape = f"{current_df.shape[0]} rows x {current_df.shape[1]} cols"
    return f"[source={source} | shape={shape}]\n\n" + desc.to_markdown()

# 7) df_A/df_B SQL ì§ˆì˜ (DuckDB)
@tool
def sql_on_dfs(query: str) -> str:
    """
    Run DuckDB SQL over df_A and (if loaded) df_B. Tables: df_A, df_B
    Example: SELECT a.key, a.metric FROM df_A a WHERE a.metric > 100 LIMIT 20
    """
    try:
        duckdb.register("df_A", pytool.globals.get("df_A", df) if pytool.globals.get("df_A") is not None else df)
        if pytool.globals.get("df_B") is not None:
            duckdb.register("df_B", pytool.globals["df_B"])
        out = duckdb.sql(query).df()
        return out.head(200).to_markdown(index=False)
    except Exception as e:
        return f"SQL error: {e}"

# 8) ì¡°ì¸ í‚¤ í›„ë³´ ì¶”ì²œ
@tool
def propose_join_keys() -> str:
    """Suggest join key candidates between df_A and df_B by intersecting column names and compatible dtypes."""
    A = pytool.globals.get("df_A") if pytool.globals.get("df_A") is not None else df
    B = pytool.globals.get("df_B")
    if B is None:
        return "df_B is not loaded."

    def dtype_sig(series):
        t = str(series.dtype)
        if "int" in t: return "int"
        if "float" in t: return "float"
        if "datetime" in t or "date" in t or "time" in t: return "datetime"
        return "str"

    pairs = []
    for c in A.columns:
        if c in B.columns and dtype_sig(A[c]) == dtype_sig(B[c]):
            pairs.append((c, dtype_sig(A[c])))

    if not pairs:
        return "No obvious same-name & same-type keys. Consider mapping tables or casting."

    md = "| key | dtype |\n|---|---|\n" + "\n".join([f"| {k} | {t} |" for k, t in pairs])
    return f"Candidate join keys (same name & type):\n{md}"

# 9) íƒ€ì„ë²„í‚· ì •ë ¬
@tool
def align_time_buckets(target: str = "A", column: str = "ts", freq: str = "H") -> str:
    """
    Resample time-like column to buckets and store as df_A_bucketed or df_B_bucketed.
    target: 'A' or 'B'; column must be timestamp-like; freq like 'H','D','15min'
    """
    t = (target or "A").strip().upper()
    cur = None
    if t == "B":
        cur = pytool.globals.get("df_B")
        if cur is None:
            return "df_B is not loaded."
    else:
        cur = pytool.globals.get("df_A") if pytool.globals.get("df_A") is not None else df

    if column not in cur.columns:
        return f"Column '{column}' not in df_{t}. Available: {list(cur.columns)}"

    tmp = cur.copy()
    tmp[column] = pd.to_datetime(tmp[column], errors="coerce")
    bucket_col = f"{column}_bucket"
    tmp[bucket_col] = tmp[column].dt.to_period(freq).dt.to_timestamp()

    key = f"df_{t}_bucketed"
    pytool.globals[key] = tmp
    prev = tmp[[bucket_col]].head().to_markdown(index=False)
    return f"Created {key} with '{bucket_col}' at freq={freq}.\nPreview:\n{prev}"

# ğŸ”Ÿ (ì‹ ê·œ) df_A/df_B ë¹„êµ ìš”ì•½ â€” ì•ˆì „í•œ í‚¤ ì „ì²˜ë¦¬ í¬í•¨
@tool
def compare_on_keys(keys: str, how: str = "inner", atol: float = 0.0, rtol: float = 0.0) -> str:
    """
    Join df_A & df_B on comma-separated `keys`, then compare shared columns.
    Returns an overview with numeric deltas & categorical match rates.
    Creates global 'df_join' for follow-up analysis (e.g., plotting, mismatch_report).
    Accepts robust inputs like: "machineID,datetime" or "keys='machineID,datetime'".
    """
    A = pytool.globals.get("df_A") if pytool.globals.get("df_A") is not None else df
    B = pytool.globals.get("df_B")
    if B is None:
        return "df_B is not loaded."

    # âœ… ì‚¬ìš©ìê°€ "keys='k1,k2'" ê°™ì´ ë„˜ê²¨ë„ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
    ks = (keys or "").strip()
    if ks.lower().startswith("keys="):
        ks = ks.split("=", 1)[1].strip()
    if (ks.startswith("'") and ks.endswith("'")) or (ks.startswith('"') and ks.endswith('"')):
        ks = ks[1:-1]
    key_cols = [k.strip() for k in ks.split(",") if k.strip()]

    if not key_cols:
        return "Please provide one or more keys (comma-separated)."
    for k in key_cols:
        if k not in A.columns or k not in B.columns:
            return f"Key '{k}' not found in both dataframes."

    # ê³µí†µ ë¹„êµ ì»¬ëŸ¼
    shared_cols = [c for c in A.columns if c in B.columns and c not in key_cols]
    A_ = A[key_cols + shared_cols].copy()
    B_ = B[key_cols + shared_cols].copy()
    A_.columns = [*key_cols] + [f"{c}__A" for c in shared_cols]
    B_.columns = [*key_cols] + [f"{c}__B" for c in shared_cols]

    df_join = pd.merge(A_, B_, on=key_cols, how=how)
    pytool.globals["df_join"] = df_join
    duckdb.register("df_join", df_join)

    # ë¹„êµ ìš”ì•½
    numeric = []
    categorical = []
    for c in shared_cols:
        a = df_join.get(f"{c}__A")
        b = df_join.get(f"{c}__B")
        if a is None or b is None:
            continue
        if pd.api.types.is_numeric_dtype(a) and pd.api.types.is_numeric_dtype(b):
            diff = (a - b).astype("float64")
            eq = (diff.abs() <= (atol + rtol * b.abs().fillna(0))).fillna(False)
            numeric.append({
                "column": c,
                "count": int(diff.notna().sum()),
                "mean_A": float(a.mean(skipna=True)),
                "mean_B": float(b.mean(skipna=True)),
                "mean_diff": float(diff.mean(skipna=True)),
                "abs_mean_diff": float(diff.abs().mean(skipna=True)),
                "pct_equal_with_tol": float(eq.mean() * 100.0),
            })
        else:
            eq = (a.astype("string") == b.astype("string"))
            categorical.append({
                "column": c,
                "count": int(eq.notna().sum()),
                "match_rate_%": float(eq.mean(skipna=True) * 100.0),
                "n_unique_A": int(a.nunique(dropna=True)),
                "n_unique_B": int(b.nunique(dropna=True)),
            })

    out = [f"[compare_on_keys] how={how}, rows={len(df_join)}, keys={key_cols}"]
    if numeric:
        df_num = pd.DataFrame(numeric).sort_values("abs_mean_diff", ascending=False)
        out.append("**Numeric comparison (sorted by abs_mean_diff):**\n" + df_num.head(50).to_markdown(index=False))
    else:
        out.append("**Numeric comparison:** None")
    if categorical:
        df_cat = pd.DataFrame(categorical).sort_values("match_rate_%")
        out.append("**Categorical comparison (lowest match first):**\n" + df_cat.head(50).to_markdown(index=False))
    else:
        out.append("**Categorical comparison:** None")
    out.append("\nTip: ì‹œê°í™”ê°€ í•„ìš”í•˜ë©´ python_repl_astì—ì„œ df_joinì„ ì‚¬ìš©í•´ ë°•ìŠ¤í”Œë¡¯/íˆìŠ¤í† ê·¸ë¨ì„ ê·¸ë¦¬ì„¸ìš”.")
    return "\n\n".join(out)

# 11) (ì‹ ê·œ) íŠ¹ì • ì»¬ëŸ¼ mismatch drilldown
@tool
def mismatch_report(column: str, top_k: int = 20) -> str:
    """
    On df_join (from compare_on_keys), show largest absolute differences (numeric)
    or most frequent mismatches (categorical). Returns top_k rows.
    """
    dj = pytool.globals.get("df_join")
    if dj is None:
        return "df_join not found. Run compare_on_keys() first."
    colA = f"{column}__A"
    colB = f"{column}__B"
    if colA not in dj.columns or colB not in dj.columns:
        return f"Column '{column}' not found in df_join."
    a, b = dj[colA], dj[colB]
    if pd.api.types.is_numeric_dtype(a) and pd.api.types.is_numeric_dtype(b):
        d = (a - b).abs()
        res = dj.assign(abs_diff=d).sort_values("abs_diff", ascending=False).head(top_k)
        return res.to_markdown(index=False)
    else:
        neq = dj[a.astype("string") != b.astype("string")]
        if len(neq) == 0:
            return "No mismatches."
        counts = (
            neq[[colA, colB]]
            .astype("string")
            .value_counts()
            .reset_index(name="count")
            .head(top_k)
        )
        return counts.to_markdown(index=False)

tools = [
    pytool,
    load_loading_csv,
    describe_columns,
    save_plots_zip,
    load_df_b,
    describe_columns_on,
    sql_on_dfs,
    propose_join_keys,
    align_time_buckets,
    compare_on_keys,     # âœ… ë¹„êµ íˆ´
    mismatch_report,     # âœ… ë“œë¦´ë‹¤ìš´ íˆ´
]

# =============================
# í™˜ê²½ ë³€ìˆ˜ í™•ì¸
# =============================
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
if not GOOGLE_API_KEY:
    st.error("GOOGLE_API_KEYê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. .env ë˜ëŠ” í™˜ê²½ë³€ìˆ˜ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
    st.stop()

# =============================
# ğŸ¤– LLM ì„¸íŒ… (ê²½ê³  ë°©ì§€: disable_streaming=False)
# =============================
llm = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash",
    api_key=GOOGLE_API_KEY,
    temperature=0,
    max_tokens=None,
    timeout=None,
    max_retries=2,
    disable_streaming=False,  # streaming=True ëŒ€ì‹  ê¶Œì¥
)

# =============================
# ğŸ“œ Prompt (ì´ìƒì  + ë‘ CSV ë¹„êµ ì›Œí¬í”Œë¡œìš°)
# =============================
df_head_txt = st.session_state["df_A_data"].head().to_string(index=False)
df_b_head_txt = st.session_state["df_B_data"].head().to_string(index=False) if st.session_state["df_B_data"] is not None else "(df_B not loaded)"

react_prompt = ChatPromptTemplate.from_messages([
    ("system",
     "You are working with pandas dataframes. The main dataframe is `df_A` (alias: df). "
     "You may also load or use a related dataframe as `df_B` for comparison & root-cause analysis.\n\n"
     "Tools available:\n{tools}\nUse only tool names from: {tool_names}.\n"
     "If you use sql_on_dfs, available tables are df_A and (if loaded) df_B; also df_join after compare_on_keys.\n\n"
     "Tool routing guide:\n"
     "- schema/summary/initial outlier bounds â†’ describe_columns or describe_columns_on\n"
     "- load file â†’ load_loading_csv or load_df_b\n"
     "- SQL/join/aggregation â†’ sql_on_dfs\n"
     "- **two-CSV comparison** â†’ **propose_join_keys â†’ compare_on_keys(keys='...') â†’ mismatch_report(column='...')**\n"
     "- **custom compute/plots/outlier analysis** â†’ **python_repl_ast**\n"
     "- align timestamps to buckets â†’ align_time_buckets\n\n"
     "**âš ï¸ ì´ìƒì (Outlier) ê²€í†  ì§€ì¹¨:**\n"
     "1. describe_columnsë¡œ ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ ê²½ê³„(min/max/std/IQR) í™•ì¸\n"
     "2. python_repl_astë¡œ Box Plot ì‹œê°í™”\n"
     "3. IQRê¸°ë°˜ ì´ìƒì  í•„í„°ë§ ê³„ì‚°\n\n"
     "**ğŸš¨ Python ì‹¤í–‰ í•„ìˆ˜ ì§€ì¹¨ (ë³€ìˆ˜ ì§€ì†ì„± ì˜¤ë¥˜ ë°©ì§€):**\n"
     "1. ë³µì¡í•œ ì‘ì—…ì€ í•˜ë‚˜ì˜ python_repl_ast í˜¸ì¶œ ë‚´ì—ì„œ ì²˜ë¦¬ (ë§ˆì§€ë§‰ì— print(...))\n"
     "2. ì½”ë“œ ìŠ¤ë‹ˆí«ì„ ì—¬ëŸ¬ ë²ˆ ë¶„í• í•˜ì§€ ë§ ê²ƒ\n\n"
     "Parsing recovery:\n"
     "- If you output 'Action:' without 'Action Input:', immediately continue with only 'Action Input: <...>'.\n"
     "- Do not wrap code in backticks.\n"
     "- Keep Action Input minimal, valid, and executable.\n\n"
     "ALWAYS follow this exact format:\n"
     "Question: <restated question>\n"
     "Thought: <brief reasoning>\n"
     "Action: <ONE tool name from {tool_names}>\n"
     "Action Input: <valid code with NO backticks>\n"
     "Observation: <tool result>\n"
     "(Repeat Thought/Action/Action Input/Observation as needed)\n"
     "Thought: I now know the final answer\n"
     "Final Answer: <answer>\n\n"
     f"df_A.head():\n{df_head_txt}\n\n"
     f"df_B.head():\n{df_b_head_txt}\n"
    ),
    MessagesPlaceholder("chat_history", optional=True),
    ("human", "{input}"),
    ("assistant", "{agent_scratchpad}"),
])

# =============================
# âš™ï¸ ReAct Agent
# =============================
react_runnable = create_react_agent(llm, tools, prompt=react_prompt)
agent = AgentExecutor(
    agent=react_runnable,
    tools=tools,
    verbose=True,
    return_intermediate_steps=True,
    handle_parsing_errors=(
        "FORMAT REMINDER: After 'Action:' you MUST output\n"
        "Action Input: <valid code with NO backticks>\n"
        "If you omitted it, continue by providing Action Input only."
    ),
)

# =============================
# ğŸ”„ RunnableWithMessageHistory
# =============================
agent_with_history = RunnableWithMessageHistory(
    agent,
    lambda session_id: history,
    input_messages_key="input",
    history_messages_key="chat_history",
)

# =============================
# ğŸ’» UI ë° ì‹¤í–‰ ë¡œì§
# =============================
st.write("---")
user_q = st.chat_input(
    "ì˜ˆ) 'propose_join_keys', \"compare_on_keys(keys='machineID,datetime')\", "
    "\"mismatch_report(column='telemetry_value')\", 'describe_columns_on(target=\"B\")'"
)

if user_q:
    # ì±„íŒ… íˆìŠ¤í† ë¦¬ ê¸°ë¡ì€ StreamlitCallbackHandlerê°€ ëŒ€ì²´
    left, right = st.columns([1, 1])
    with left:
        st.subheader("ì‹¤ì‹œê°„ ì‹¤í–‰ ë¡œê·¸")
        # StreamlitCallbackHandlerëŠ” ì¤‘ê°„ ë‹¨ê³„ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ë³´ì—¬ì¤ë‹ˆë‹¤.
        st_cb = StreamlitCallbackHandler(st.container())

    # ì‚¬ìš©ì ì •ì˜ ì½œë°± (íƒ€ì„ë¼ì¸ ë””ë²„ê¹…ìš©)
    collector = SimpleCollectCallback()

    with st.spinner("Thinking with Gemini..."):
        try:
            result = agent_with_history.invoke(
                {"input": user_q},
                {
                    # StdOutCallbackHandlerëŠ” í„°ë¯¸ë„ì— ë¡œê·¸ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.
                    "callbacks": [st_cb, collector, StdOutCallbackHandler()],
                    "configurable": {"session_id": "konlo_ssid"},
                }
            )
        except Exception as e:
            st.error(f"Agent ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            result = {"output": f"Agent ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"}

    st.success("Done.")

    final = result.get("output", "Agentê°€ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
    with right:
        st.subheader("Answer")
        # LLMì˜ ìµœì¢… ë‹µë³€ì„ ì±„íŒ…ì°½ì— í‘œì‹œ
        st.write(final)

        # í”Œë¡¯ ë Œë”ë§
        figs = [plt.figure(n) for n in plt.get_fignums()]
        for f in figs:
            f.set_size_inches(6, 4)
            f.set_dpi(100)
            st.pyplot(f, use_container_width=True)
        plt.close("all")

    # intermediate steps (ë””ë²„ê¹… ì •ë³´)
    st.write("---")
    st.subheader("intermediate_steps (íˆ´ ì‹¤í–‰ ìƒì„¸)")
    steps = result.get("intermediate_steps", [])
    if steps:
        for i, (action, observation) in enumerate(steps, 1):
            with st.expander(f"Step {i}: {action.tool}"):
                st.markdown("**tool_input**")
                st.code(str(action.tool_input))
                st.markdown("**observation**")
                st.code(observation)
    else:
        st.info("intermediate_steps ë¹„ì–´ ìˆìŒ")

    # ì½œë°± íƒ€ì„ë¼ì¸ (ë””ë²„ê¹… ì •ë³´)
    st.write("---")
    st.subheader("ì½œë°± ì´ë²¤íŠ¸ íƒ€ì„ë¼ì¸ (Simple Collect)")
    if collector.events:
        for j, e in enumerate(collector.events, 1):
            with st.expander(f"Event {j}: {e.get('type')}"):
                if e.get("type") == "llm_start":
                    prompts = e.get("prompts", [])
                    for k, p in enumerate(prompts, 1):
                        st.markdown(f"**Prompt {k}**")
                        st.code(p)
                else:
                    st.write(e)
    else:
        st.info("ì½œë°± ì´ë²¤íŠ¸ ì •ë³´ ì—†ìŒ")

# =============================
# Streamlit ì±„íŒ… ê¸°ë¡ í‘œì‹œ
# =============================
st.sidebar.title("Chat History")
for msg in history.messages:
    if msg.type == "human":
        with st.sidebar:
            st.chat_message("user").write(msg.content)
    elif msg.type == "ai":
        # AI ë©”ì‹œì§€ëŠ” ìµœì¢… ë‹µë³€ì´ ì´ë¯¸ ë©”ì¸ì— í‘œì‹œë˜ì—ˆìœ¼ë¯€ë¡œ, ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ í‘œì‹œ
        with st.sidebar:
            st.chat_message("assistant").write(msg.content[:50] + "...")
