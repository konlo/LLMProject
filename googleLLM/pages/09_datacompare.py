import os
import sys
import io
import zipfile
from typing import Any, Dict, List, Optional

import pandas as pd
import numpy as np
import streamlit as st
from dotenv import load_dotenv
import matplotlib.pyplot as plt
import duckdb

# Optional deps
try:
    from sklearn.ensemble import IsolationForest
except Exception:
    IsolationForest = None

try:
    from statsmodels.tsa.seasonal import STL
except Exception:
    STL = None

# LangChain & Gemini
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_community.callbacks.streamlit import StreamlitCallbackHandler
from langchain_community.chat_message_histories import StreamlitChatMessageHistory
from langchain_experimental.tools.python.tool import PythonAstREPLTool
from langchain.agents import create_react_agent, AgentExecutor
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.tools import tool
from langchain_core.callbacks.base import BaseCallbackHandler
from langchain.callbacks import StdOutCallbackHandler
from langchain_core.tools import render_text_description  # for {tools}/{tool_names}

# =======================================================================
# âœ… ê³µìš© íŒŒì„œ ìœ í‹¸ (ë¹ˆ ë¬¸ìì—´/None/ë¬¸ìì—´ ìˆ«ìë„ ì•ˆì „ ë³€í™˜)
# =======================================================================
def _parse_int(val, default: int) -> int:
    """ë¹ˆ ë¬¸ìì—´/None/ë¬¸ìì—´/ì •ìˆ˜ ëª¨ë‘ ì•ˆì „ ë³€í™˜."""
    if val is None:
        return default
    if isinstance(val, (int, np.integer)):
        return int(val)
    if isinstance(val, float):
        try:
            return int(val)
        except Exception:
            return default
    if isinstance(val, str):
        s = val.strip()
        if s == "":
            return default
        try:
            return int(float(s))
        except Exception:
            return default
    return default

def _parse_float(val, default: float) -> float:
    """ë¹ˆ ë¬¸ìì—´/None/ë¬¸ìì—´/ì‹¤ìˆ˜ ëª¨ë‘ ì•ˆì „ ë³€í™˜."""
    if val is None:
        return default
    if isinstance(val, (int, float, np.integer, np.floating)):
        try:
            return float(val)
        except Exception:
            return default
    if isinstance(val, str):
        s = val.strip()
        if s == "":
            return default
        try:
            return float(s)
        except Exception:
            return default
    return default

# =======================================================================
# ğŸ› ï¸ ì½œë°± ìˆ˜ì§‘ê¸°
# =======================================================================
class SimpleCollectCallback(BaseCallbackHandler):
    """
    LangChain AgentExecutorì™€ì˜ í˜¸í™˜ì„±ì„ ìœ„í•´ BaseCallbackHandler ìƒì†.
    ì¤‘ê°„ ì´ë²¤íŠ¸/ì—ëŸ¬ë¥¼ ìˆ˜ì§‘í•˜ì—¬ UIì— ë…¸ì¶œ.
    """
    def __init__(self):
        self.events = []
    
    def on_tool_error(self, error: Exception, **kwargs: Any) -> Any:
        self.events.append({"type": "tool_error", "error": str(error)})
        
    def on_llm_start(self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any) -> None:
        self.events.append({"type": "llm_start", "prompts": prompts})
        
    def on_llm_end(self, response: Any, **kwargs: Any) -> None:
        self.events.append({"type": "llm_end"})

    def on_agent_action(self, action: Any, **kwargs: Any) -> Any:
        pass
    
    def on_agent_finish(self, finish: Any, **kwargs: Any) -> Any:
        pass

# =============================
# ğŸš€ App ì‹œì‘ & ì „ì—­ ì„¤ì •
# =============================
load_dotenv()
st.set_page_config(page_title="DF Chatbot (Gemini)", page_icon="âœ¨", layout="wide")
st.title("âœ¨ DataFrame Chatbot (Gemini + LangChain)")
st.caption("ë‘ CSV ë¹„êµ + ì´ìƒì  ì¤‘ì‹¬ EDA(ì›í´ë¦­) + SSD Telemetry ìœ í‹¸")

# =============================
# ğŸ“ ë°ì´í„° ë¡œë“œ & ì„¸ì…˜ ìƒíƒœ
# =============================
DEFAULT_DATA_DIR = os.getenv("DATA_DIR", "/Users/najongseong/dataset")
DFB_DEFAULT_NAME = "telemetry_raw.csv"  # df_B ê¸°ë³¸ íŒŒì¼ëª…
SUPPORTED_EXTENSIONS = (".csv", ".parquet")

def _init_session_state():
    for key, default in [
        ("DATA_DIR", DEFAULT_DATA_DIR),
        ("df_A_data", None),
        ("df_A_name", "No Data"),
        ("csv_path", os.path.join(DEFAULT_DATA_DIR, "stormtrooper.csv")),
        ("df_B_data", None),
        ("df_B_name", "No Data"),
        ("csv_b_path", ""),
    ]:
        if key not in st.session_state:
            st.session_state[key] = default

_init_session_state()

def _read_table(path: str) -> pd.DataFrame:
    """í™•ì¥ìì— ë§ê²Œ CSV ë˜ëŠ” Parquetë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    ext = os.path.splitext(path)[1].lower()
    if ext == ".csv":
        return pd.read_csv(path)
    if ext == ".parquet":
        return pd.read_parquet(path)
    raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤: {ext}")

def load_df_A(path: str, display_name: str):
    """ì§€ì •ëœ ê²½ë¡œì—ì„œ df_Aë¥¼ ë¡œë“œí•˜ê³  ì„¸ì…˜ ìƒíƒœë¥¼ ì—…ë°ì´íŠ¸."""
    try:
        new_df = _read_table(path)
        st.session_state["df_A_data"] = new_df
        st.session_state["df_A_name"] = display_name
        st.session_state["csv_path"] = path 
        return True, f"Loaded file: {display_name} (Shape: {new_df.shape})"
    except Exception as e:
        st.session_state["df_A_data"] = None
        st.session_state["df_A_name"] = "Load Failed"
        st.session_state["csv_path"] = "" 
        return False, f"ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {path}\n{e}"

def load_df_B(path: str, display_name: str):
    """ì§€ì •ëœ ê²½ë¡œì—ì„œ df_Bë¥¼ ë¡œë“œí•˜ê³  ì„¸ì…˜ ìƒíƒœë¥¼ ì—…ë°ì´íŠ¸."""
    try:
        new_df = _read_table(path)
        st.session_state["df_B_data"] = new_df
        st.session_state["df_B_name"] = display_name
        st.session_state["csv_b_path"] = path 
        return True, f"Loaded file (df_B): {display_name} (Shape: {new_df.shape})"
    except Exception as e:
        st.session_state["df_B_data"] = None
        st.session_state["df_B_name"] = "Load Failed"
        st.session_state["csv_b_path"] = "" 
        return False, f"df_B ë¡œë“œ ì‹¤íŒ¨: {path}\n{e}"

# =============================
# ğŸ“š ì‚¬ì´ë“œë°”: í´ë”/íŒŒì¼ ì„ íƒ
# =============================
with st.sidebar:
    st.markdown("### ğŸ—‚ï¸ 1. ë°ì´í„° í´ë” ì„¤ì •")
    new_data_dir = st.text_input("Enter Data Directory Path",
                                 value=st.session_state["DATA_DIR"],
                                 key="data_dir_input")
    if st.button("Set Directory"):
        if os.path.isdir(new_data_dir):
            st.session_state["DATA_DIR"] = new_data_dir
            st.session_state["df_A_data"] = None
            st.session_state["df_A_name"] = "No Data"
            st.session_state["csv_path"] = "" 
            st.session_state["df_B_data"] = None
            st.session_state["df_B_name"] = "No Data"
            st.session_state["csv_b_path"] = "" 
            st.success(f"Directory set to: `{new_data_dir}`")
            st.rerun()
        else:
            st.error(f"Invalid directory path: `{new_data_dir}`")

    DATA_DIR = st.session_state["DATA_DIR"]
    DFB_DEFAULT = os.path.join(DATA_DIR, DFB_DEFAULT_NAME)

    st.markdown("---")
    st.markdown("### ğŸ“„ 2. df_A íŒŒì¼ ì„ íƒ")
    st.caption(f"Search directory: `{DATA_DIR}`")
    data_files: List[str] = []
    try:
        if os.path.isdir(DATA_DIR):
            for f in os.listdir(DATA_DIR):
                if f.lower().endswith(SUPPORTED_EXTENSIONS):
                    data_files.append(f)
            data_files.sort()
        else:
            st.warning("ìœ íš¨í•œ ë°ì´í„° ë””ë ‰í† ë¦¬ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
    except Exception as e:
        st.error(f"í´ë” ì ‘ê·¼ ì˜¤ë¥˜: {e}")

    selected_file = st.selectbox("Select data file for df_A",
                                 options=["--- Select a file ---"] + data_files,
                                 key="file_selector")
    if st.button("Load Selected File (df_A)"):
        if selected_file and selected_file != "--- Select a file ---":
            file_path = os.path.join(DATA_DIR, selected_file)
            success, message = load_df_A(file_path, selected_file)
            if success:
                st.success(message)
                st.rerun()
            else:
                st.error(message)
        else:
            st.warning("df_A íŒŒì¼ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")

    st.markdown("---")
    st.markdown("### ğŸ“„ 3. df_B íŒŒì¼ ì„ íƒ (ë¹„êµìš©)")
    selected_file_b = st.selectbox("Select data file for df_B",
                                   options=["--- Select a file ---"] + data_files,
                                   key="file_selector_b")
    if st.button("Load Selected File (df_B)"):
        if selected_file_b and selected_file_b != "--- Select a file ---":
            file_path_b = os.path.join(DATA_DIR, selected_file_b)
            success, message = load_df_B(file_path_b, selected_file_b)
            if success:
                st.success(message)
                st.rerun()
            else:
                st.error(message)
        else:
            st.warning("df_B íŒŒì¼ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")

    st.markdown("---")
    st.caption(f"**í˜„ì¬ ë¡œë“œ íŒŒì¼ ê²½ë¡œ(df_A):** `{st.session_state.get('csv_path', 'Not loaded')}`")
    st.caption(f"**í˜„ì¬ ë¡œë“œ íŒŒì¼ ê²½ë¡œ(df_B):** `{st.session_state.get('csv_b_path', 'Not loaded')}`")
    st.caption(f"df_B ê¸°ë³¸ ê°€ì • íŒŒì¼: `{os.path.basename(DFB_DEFAULT)}`")

# ìµœì¢… df ê²°ì •
df_A: Optional[pd.DataFrame] = st.session_state["df_A_data"]
df_B: Optional[pd.DataFrame] = st.session_state["df_B_data"]

if df_A is None:
    st.error("ë¶„ì„í•  DataFrame (df_A)ì„ ë¡œë“œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ìœ íš¨í•œ ë””ë ‰í† ë¦¬ì™€ ì§€ì›ë˜ëŠ” ë°ì´í„° íŒŒì¼ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")
    st.stop()

# =============================
# ğŸ–¼ï¸ Preview
# =============================
st.subheader("Preview")
st.write(f"**Loaded file for df_A:** `{st.session_state['df_A_name']}` (Shape: {df_A.shape})")
st.dataframe(df_A.head(10), width="stretch")
if df_B is not None:
    with st.expander(f"df_B Preview â€” {st.session_state['df_B_name']} (Shape: {df_B.shape})", expanded=False):
        st.dataframe(df_B.head(10), width="stretch")

# =============================
# ğŸ’¬ Chat history
# =============================
history = StreamlitChatMessageHistory(key="lc_msgs:dfchat")

# =============================
# ğŸ› ï¸ Tools ì •ì˜
# =============================
pytool = PythonAstREPLTool(
    globals={
        "pd": pd,
        "np": np,
        "plt": plt,
        "df": df_A,      # alias
        "df_A": df_A,    # main
        "df_B": df_B,    # secondary (optional)
        "df_join": None,
        "duckdb": duckdb,
    },
    name="python_repl_ast",
    description="Execute Python on df_A/df_B/df_join with pandas/matplotlib."
)

@tool
def load_loading_csv(filename: str) -> str:
    """Load a CSV or Parquet from DATA_DIR into 'loading_df'. Pass only file name."""
    current_data_dir = st.session_state.get("DATA_DIR", DEFAULT_DATA_DIR)
    path = os.path.join(current_data_dir, filename)
    try:
        new_df = _read_table(path)
    except Exception as e:
        return f"Failed to load {path}: {e}"
    pytool.globals["loading_df"] = new_df
    preview = new_df.head(10).to_markdown(index=False)
    shape = f"{new_df.shape[0]} rows x {new_df.shape[1]} cols"
    return f"Loaded {filename} from {current_data_dir} into loading_df\nShape: {shape}\n\nPreview (head):\n{preview}"

@tool
def describe_columns(cols: str = "") -> str:
    """
    Describe selected columns (comma-separated).
    Uses 'loading_df' if available; otherwise uses 'df_A'.
    """
    if "loading_df" in pytool.globals and pytool.globals["loading_df"] is not None:
        current_df = pytool.globals["loading_df"]
        source = "loading_df"
    else:
        current_df = pytool.globals.get("df_A")
        source = "df_A"
    if current_df is None:
        return "df_A not loaded."
    use_cols = [c.strip() for c in (cols or "").split(",") if c.strip()] or list(current_df.columns)
    missing = [c for c in use_cols if c not in current_df.columns]
    if missing:
        return f"Missing columns: {missing}\nAvailable: {list(current_df.columns)}"
    desc = current_df[use_cols].describe(include="all").transpose()
    shape = f"{current_df.shape[0]} rows x {current_df.shape[1]} cols"
    return f"[source={source} | shape={shape}]\n\n" + desc.to_markdown()

@tool
def save_plots_zip() -> str:
    """Zip all current matplotlib figures. Use after plotting with python_repl_ast."""
    figs = [plt.figure(n) for n in plt.get_fignums()]
    if not figs:
        return "No figures to save."
    buf = io.BytesIO()
    with zipfile.ZipFile(buf, "w", zipfile.ZIP_DEFLATED) as zf:
        for i, f in enumerate(figs, 1):
            img = io.BytesIO()
            f.savefig(img, format="png", dpi=110, bbox_inches="tight")
            img.seek(0)
            zf.writestr(f"plot_{i}.png", img.read())
    return f"Zipped {len(figs)} plots. Bytes={len(buf.getvalue())}"

@tool
def load_df_b(filename: str = "") -> str:
    """
    Load a CSV or Parquet into 'df_B'. If empty, defaults to telemetry_raw.csv under DATA_DIR.
    """
    current_data_dir = st.session_state.get("DATA_DIR", DEFAULT_DATA_DIR)
    if not filename:
        path = os.path.join(current_data_dir, DFB_DEFAULT_NAME)
        show_name = os.path.basename(path)
    else:
        path = os.path.join(current_data_dir, filename) if not os.path.isabs(filename) else filename
        show_name = os.path.basename(path)
    try:
        new_df = _read_table(path)
    except Exception as e:
        return f"Failed to load df_B from {path}: {e}"
    pytool.globals["df_B"] = new_df
    preview = new_df.head(10).to_markdown(index=False)
    return f"Loaded df_B from '{show_name}' (full: {path}) with shape {new_df.shape}\n\nPreview:\n{preview}"

@tool
def describe_columns_on(target: str = "A", cols: str = "") -> str:
    """
    Describe columns from df_A or df_B. target: 'A' | 'B'; cols: comma-separated; empty -> all
    """
    t = (target or "A").strip().upper()
    if t == "B":
        cur = pytool.globals.get("df_B")
        if cur is None:
            return "df_B is not loaded. Use load_df_b() or sidebar."
        source = "df_B"
    else:
        cur = pytool.globals.get("df_A")
        source = "df_A"
    if cur is None:
        return "df_A not loaded."
    use_cols = [c.strip() for c in (cols or "").split(",") if c.strip()] or list(cur.columns)
    missing = [c for c in use_cols if c not in cur.columns]
    if missing:
        return f"Missing columns: {missing}\nAvailable: {list(cur.columns)}"
    desc = cur[use_cols].describe(include="all").transpose()
    return f"[source={source} | shape={cur.shape}]\n\n" + desc.to_markdown()

@tool
def sql_on_dfs(query: str) -> str:
    """
    Run DuckDB SQL over df_A, df_B (if loaded), df_join (if created).
    Tables: df_A, df_B, df_join
    """
    try:
        if pytool.globals.get("df_A") is not None:
            duckdb.register("df_A", pytool.globals["df_A"])
        if pytool.globals.get("df_B") is not None:
            duckdb.register("df_B", pytool.globals["df_B"])
        if pytool.globals.get("df_join") is not None:
            duckdb.register("df_join", pytool.globals["df_join"])
        out = duckdb.sql(query).df()
        return out.head(200).to_markdown(index=False)
    except Exception as e:
        return f"SQL error: {e}"

@tool
def propose_join_keys() -> str:
    """Suggest same-name & compatible-dtype join key candidates between df_A and df_B."""
    A = pytool.globals.get("df_A")
    B = pytool.globals.get("df_B")
    if A is None:
        return "df_A not loaded."
    if B is None:
        return "df_B is not loaded."
    def dtype_sig(series):
        t = str(series.dtype)
        if "int" in t: return "int"
        if "float" in t: return "float"
        if "datetime" in t or "date" in t or "time" in t: return "datetime"
        return "str"
    pairs = []
    for c in A.columns:
        if c in B.columns and dtype_sig(A[c]) == dtype_sig(B[c]):
            pairs.append((c, dtype_sig(A[c])))
    if not pairs:
        return "No obvious same-name & same-type keys. Consider casting or mapping."
    md = "| key | dtype |\n|---|---|\n" + "\n".join([f"| {k} | {t} |" for k, t in pairs])
    return f"Candidate join keys:\n{md}"

@tool
def align_time_buckets(target: str = "A", column: str = "datetime", freq: str = "H") -> str:
    """
    Resample time-like column to buckets and store as df_A_bucketed or df_B_bucketed.
    freq like 'H','D','15min'
    """
    t = (target or "A").strip().upper()
    cur = pytool.globals.get("df_B") if t == "B" else pytool.globals.get("df_A")
    if cur is None:
        return f"df_{t} not loaded."
    if column not in cur.columns:
        return f"Column '{column}' not in df_{t}."
    tmp = cur.copy()
    tmp[column] = pd.to_datetime(tmp[column], errors="coerce")
    bucket_col = f"{column}_bucket"
    tmp[bucket_col] = tmp[column].dt.to_period(freq).dt.to_timestamp()
    pytool.globals[f"df_{t}_bucketed"] = tmp
    prev = tmp[[bucket_col]].head().to_markdown(index=False)
    return f"Created df_{t}_bucketed with '{bucket_col}' at freq={freq}.\nPreview:\n{prev}"

# ---------- ë¹„êµ ìœ í‹¸ ----------
@tool
def compare_on_keys(keys: str, how: str = "inner", atol: Any = 0.0, rtol: Any = 0.0) -> str:
    """
    Join df_A & df_B on comma-separated `keys`, then compare shared columns.
    Robust to inputs like: "machineID,datetime" or "keys='machineID,datetime'".
    Creates global 'df_join'.
    """
    A = pytool.globals.get("df_A")
    B = pytool.globals.get("df_B")
    if A is None:
        return "df_A not loaded."
    if B is None:
        return "df_B is not loaded."

    # tol íŒŒì‹±
    atol_val = _parse_float(atol, 0.0)
    rtol_val = _parse_float(rtol, 0.0)

    ks = (keys or "").strip()
    if ks.lower().startswith("keys="):
        ks = ks.split("=", 1)[1].strip()
    if (ks.startswith("'") and ks.endswith("'")) or (ks.startswith('"') and ks.endswith('"')):
        ks = ks[1:-1]
    key_cols = [k.strip() for k in ks.split(",") if k.strip()]
    if not key_cols:
        return "Please provide one or more keys (comma-separated)."
    for k in key_cols:
        if k not in A.columns or k not in B.columns:
            return f"Key '{k}' not found in both dataframes."

    shared_cols = [c for c in A.columns if c in B.columns and c not in key_cols]
    A_ = A[key_cols + shared_cols].copy()
    B_ = B[key_cols + shared_cols].copy()
    A_.columns = [*key_cols] + [f"{c}__A" for c in shared_cols]
    B_.columns = [*key_cols] + [f"{c}__B" for c in shared_cols]

    df_join = pd.merge(A_, B_, on=key_cols, how=how)
    pytool.globals["df_join"] = df_join
    try:
        duckdb.register("df_join", df_join)
    except Exception:
        pass

    numeric, categorical = [], []
    for c in shared_cols:
        a = df_join.get(f"{c}__A")
        b = df_join.get(f"{c}__B")
        if a is None or b is None:
            continue
        if pd.api.types.is_numeric_dtype(a) and pd.api.types.is_numeric_dtype(b):
            diff = (a - b).astype("float64")
            eq = (diff.abs() <= (atol_val + rtol_val * b.abs().fillna(0))).fillna(False)
            numeric.append({
                "column": c,
                "count": int(diff.notna().sum()),
                "mean_A": float(a.mean(skipna=True)),
                "mean_B": float(b.mean(skipna=True)),
                "mean_diff": float(diff.mean(skipna=True)),
                "abs_mean_diff": float(diff.abs().mean(skipna=True)),
                "pct_equal_with_tol": float(eq.mean() * 100.0),
            })
        else:
            eq = (a.astype("string") == b.astype("string"))
            categorical.append({
                "column": c,
                "count": int(eq.notna().sum()),
                "match_rate_%": float(eq.mean(skipna=True) * 100.0),
                "n_unique_A": int(a.nunique(dropna=True)),
                "n_unique_B": int(b.nunique(dropna=True)),
            })

    out = [f"[compare_on_keys] how={how}, rows={len(df_join)}, keys={key_cols}, atol={atol_val}, rtol={rtol_val}"]
    if numeric:
        df_num = pd.DataFrame(numeric).sort_values("abs_mean_diff", ascending=False)
        out.append("**Numeric comparison (top 20 by abs_mean_diff):**\n" + df_num.head(20).to_markdown(index=False))
    else:
        out.append("**Numeric comparison:** None")
    if categorical:
        df_cat = pd.DataFrame(categorical).sort_values("match_rate_%")
        out.append("**Categorical comparison (lowest 20 match first):**\n" + df_cat.head(20).to_markdown(index=False))
    else:
        out.append("**Categorical comparison:** None")
    out.append("\nTip: ì‹œê°í™”ê°€ í•„ìš”í•˜ë©´ python_repl_astì—ì„œ df_joinì„ ì‚¬ìš©í•˜ì„¸ìš”.")
    return "\n\n".join(out)

@tool
def mismatch_report(column: str, top_k: Any = 20) -> str:
    """On df_join, show largest numeric diffs or frequent categorical mismatches for a column."""
    dj = pytool.globals.get("df_join")
    if dj is None:
        return "df_join not found. Run compare_on_keys() first."

    topk_val = _parse_int(top_k, 20)

    colA = f"{column}__A"
    colB = f"{column}__B"
    if colA not in dj.columns or colB not in dj.columns:
        return f"Column '{column}' not found in df_join."
    a, b = dj[colA], dj[colB]
    if pd.api.types.is_numeric_dtype(a) and pd.api.types.is_numeric_dtype(b):
        d = (a - b).abs()
        res = dj.assign(abs_diff=d).sort_values("abs_diff", ascending=False).head(topk_val)
        return res.to_markdown(index=False)
    neq = dj[a.astype("string") != b.astype("string")]
    if len(neq) == 0:
        return "No mismatches."
    counts = (
        neq[[colA, colB]]
        .astype("string")
        .value_counts()
        .reset_index(name="count")
        .head(topk_val)
    )
    return counts.to_markdown(index=False)

# ---------- SSD Telemetry EDA ìœ í‹¸ ----------
@tool
def make_timesafe(column: str = "datetime", tz: str = "UTC") -> str:
    """
    Parse df_A/df_B[<column>] to datetime; if tz provided, localize/convert. Updates in-place.
    """
    changed = []
    for name in ["df_A", "df_B"]:
        cur = pytool.globals.get(name)
        if cur is None or column not in cur.columns:
            continue
        tmp = cur.copy()
        tmp[column] = pd.to_datetime(tmp[column], errors="coerce")
        if tz:
            try:
                if tmp[column].dt.tz is None:
                    tmp[column] = tmp[column].dt.tz_localize(tz)
                else:
                    tmp[column] = tmp[column].dt.tz_convert(tz)
            except Exception:
                pass
        pytool.globals[name] = tmp
        changed.append(f"{name}({len(tmp)} rows)")
    if not changed:
        return f"No target column '{column}' found in df_A/df_B."
    return f"[make_timesafe] column='{column}', tz='{tz}' â†’ updated: {', '.join(changed)}"

@tool
def create_features(kind: str = "wear,temp,error,perf") -> str:
    """
    Create domain features on df_A if sources exist:
    - wear: rolling std etc.
    - temp: delta & rolling p95
    - error: per TB / per hour
    - perf: WAF, tail-latency ratio
    """
    A = pytool.globals.get("df_A")
    if A is None:
        return "df_A not loaded."
    out_cols = []
    dfw = A.copy()
    kinds = {k.strip() for k in (kind or "").split(",") if k.strip()}

    if "wear" in kinds and "wear_leveling_count" in dfw.columns:
        dfw["wear_leveling_std_5"] = dfw["wear_leveling_count"].rolling(5, min_periods=1).std()
        out_cols += ["wear_leveling_std_5"]

    if "temp" in kinds and "temperature" in dfw.columns:
        dfw["temp_delta"] = dfw["temperature"].diff()
        out_cols += ["temp_delta"]
        if "datetime" in dfw.columns:
            try:
                ts = pd.to_datetime(dfw["datetime"], errors="coerce")
                dfw = dfw.assign(__ts=ts).sort_values("__ts")
                dfw["temp_p95_12"] = (
                    dfw["temperature"].rolling(12, min_periods=3)
                    .apply(lambda x: np.nanpercentile(x, 95), raw=True)
                )
                dfw = dfw.drop(columns=["__ts"])
                out_cols += ["temp_p95_12"]
            except Exception:
                pass

    if "error" in kinds:
        if "uncorrectable_error_count" in dfw.columns and "tbw" in dfw.columns:
            safe_tbw = dfw["tbw"].replace(0, np.nan)
            dfw["uncorrectable_per_tb"] = dfw["uncorrectable_error_count"] / safe_tbw
            out_cols += ["uncorrectable_per_tb"]
        if "datetime" in dfw.columns and "uncorrectable_error_count" in dfw.columns:
            ts = pd.to_datetime(dfw["datetime"], errors="coerce")
            dfw = dfw.assign(__ts=ts).sort_values("__ts")
            delta = dfw["uncorrectable_error_count"].diff()
            dt_hours = dfw["__ts"].diff().dt.total_seconds() / 3600.0
            dfw["uncorr_per_hour"] = delta / dt_hours.replace(0, np.nan)
            dfw = dfw.drop(columns=["__ts"])
            out_cols += ["uncorr_per_hour"]

    if "perf" in kinds:
        if "nand_writes" in dfw.columns and "host_writes" in dfw.columns:
            denom = dfw["host_writes"].replace(0, np.nan)
            dfw["waf"] = dfw["nand_writes"] / denom
            out_cols += ["waf"]
        if all(c in dfw.columns for c in ["latency_p50", "latency_p99"]):
            denom = dfw["latency_p50"].replace(0, np.nan)
            dfw["latency_tail_ratio"] = dfw["latency_p99"] / denom
            out_cols += ["latency_tail_ratio"]

    if not out_cols:
        return "No feature created (missing source columns)."
    pytool.globals["df_A"] = dfw
    return f"[create_features] created: {out_cols}"

@tool
def rolling_stats(cols: str, window: str = "24H", on: str = "datetime") -> str:
    """
    Rolling mean/std for numeric cols in df_A using time window like '24H','7D'.
    Saves to pytool.globals['df_A_rolling'].
    """
    A = pytool.globals.get("df_A")
    if A is None:
        return "df_A not loaded."
    targets = [c.strip() for c in (cols or "").split(",") if c.strip()]
    if not targets:
        return "Please provide cols (comma-separated)."
    for c in targets:
        if c not in A.columns:
            return f"Column '{c}' not found in df_A."
    if on not in A.columns:
        return f"Time column '{on}' not found."
    tmp = A.copy()
    tmp[on] = pd.to_datetime(tmp[on], errors="coerce")
    tmp = tmp.sort_values(on).set_index(on)
    out = pd.DataFrame(index=tmp.index)
    for c in targets:
        if pd.api.types.is_numeric_dtype(tmp[c]):
            out[f"{c}_roll_mean"] = tmp[c].rolling(window, min_periods=3).mean()
            out[f"{c}_roll_std"] = tmp[c].rolling(window, min_periods=3).std()
    out = out.reset_index().rename(columns={on: on})
    pytool.globals["df_A_rolling"] = out
    return f"[rolling_stats] window={window}, cols={targets}"

@tool
def stl_decompose(col: str, period: Any = 24, on: str = "datetime") -> str:
    """
    STL decomposition on df_A[col] with seasonal period (e.g., 24 for hourly daily).
    Saves components to pytool.globals['df_A_stl'].
    """
    if STL is None:
        return "statsmodelsê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. (pip install statsmodels)"
    A = pytool.globals.get("df_A")
    if A is None:
        return "df_A not loaded."

    # period íŒŒì‹±
    period_val = _parse_int(period, 24)

    # ì…ë ¥ ìœ ì—°í™”
    s = str(col).strip()
    try:
        if s.startswith("{") and s.endswith("}"):
            import json
            obj = json.loads(s)
            s = obj.get("col") or obj.get("column") or obj.get("name") or ""
    except Exception:
        pass
    for prefix in ("col=", "column=", "name="):
        if s.lower().startswith(prefix):
            s = s.split("=", 1)[1].strip()
            break
    if (s.startswith("'") and s.endswith("'")) or (s.startswith('"') and s.endswith('"')):
        s = s[1:-1]

    if s not in A.columns:
        return f"Column '{s}' not found."
    if on not in A.columns:
        return f"Time column '{on}' not found."
    ts = pd.to_datetime(A[on], errors="coerce")
    x = pd.to_numeric(A[s], errors="coerce")
    ok = ts.notna() & x.notna()
    ts, x = ts[ok], x[ok]
    x = pd.Series(x.values, index=ts).sort_index()
    need = max(2 * period_val, 30)
    if len(x) < need:
        return f"Not enough points for STL (need ~{need})."
    res = STL(x, period=period_val, robust=True).fit()
    out = pd.DataFrame({
        on: x.index,
        f"{s}_trend": res.trend.values,
        f"{s}_seasonal": res.seasonal.values,
        f"{s}_resid": res.resid.values,
    })
    pytool.globals["df_A_stl"] = out
    return f"[stl_decompose] period={period_val}, col='{s}'"

@tool
def anomaly_iqr(col: str) -> str:
    """
    Mark IQR-based outliers on df_A[col]. Adds '{col}_is_outlier_iqr' boolean.
    Accepts inputs like:
      - host_total_write_gb
      - col=host_total_write_gb / column=host_total_write_gb / name=host_total_write_gb
      - {"col":"host_total_write_gb"} / {"column":"host_total_write_gb"}
    """
    A = pytool.globals.get("df_A")
    if A is None:
        return "df_A not loaded."

    s = str(col).strip()
    try:
        if s.startswith("{") and s.endswith("}"):
            import json
            obj = json.loads(s)
            s = obj.get("col") or obj.get("column") or obj.get("name") or ""
    except Exception:
        pass
    for prefix in ("col=", "column=", "name="):
        if s.lower().startswith(prefix):
            s = s.split("=", 1)[1].strip()
            break
    if (s.startswith("'") and s.endswith("'")) or (s.startswith('"') and s.endswith('"')):
        s = s[1:-1]

    if s == "":
        return "Please provide a column name."
    if s not in A.columns:
        return f"Column '{s}' not found."

    x = pd.to_numeric(A[s], errors="coerce")
    q1, q3 = x.quantile(0.25), x.quantile(0.75)
    iqr = q3 - q1
    lo, hi = q1 - 1.5 * iqr, q3 + 1.5 * iqr
    flags = (x < lo) | (x > hi)
    A[f"{s}_is_outlier_iqr"] = flags
    pytool.globals["df_A"] = A
    return f"[anomaly_iqr] col='{s}', bounds=({lo:.3f},{hi:.3f}), outliers={int(flags.sum())}"

@tool
def anomaly_isoforest(cols: str, contamination: Any = 0.01, random_state: Any = 42) -> str:
    """
    IsolationForest anomalies on df_A[cols]. Adds 'isoforest_outlier' boolean.
    Accepts:
      - "temperature,waf"
      - "cols=temperature,waf"
      - {"cols": "temperature,waf"}
    """
    if IsolationForest is None:
        return "scikit-learnì´ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. (pip install scikit-learn)"
    A = pytool.globals.get("df_A")
    if A is None:
        return "df_A not loaded."

    contamination_val = _parse_float(contamination, 0.01)
    rs_val = _parse_int(random_state, 42)

    s = str(cols).strip()
    try:
        if s.startswith("{") and s.endswith("}"):
            import json
            obj = json.loads(s)
            s = obj.get("cols") or obj.get("columns") or obj.get("features") or ""
    except Exception:
        pass
    for prefix in ("cols=", "columns=", "features="):
        if s.lower().startswith(prefix):
            s = s.split("=", 1)[1].strip()
            break
    if (s.startswith("'") and s.endswith("'")) or (s.startswith('"') and s.endswith('"')):
        s = s[1:-1]

    targets = [c.strip() for c in s.split(",") if c.strip()]
    if not targets:
        return "Please provide cols."
    for c in targets:
        if c not in A.columns:
            return f"Column '{c}' not found."

    X = A[targets].apply(pd.to_numeric, errors="coerce").dropna()
    if X.shape[0] < 20:
        return "Not enough rows for IsolationForest (>=20 recommended)."
    clf = IsolationForest(n_estimators=200, contamination=contamination_val, random_state=rs_val)
    pred = clf.fit_predict(X.values)  # -1 = outlier
    flags = pd.Series(pred == -1, index=X.index)
    A["isoforest_outlier"] = False
    A.loc[flags.index, "isoforest_outlier"] = flags
    pytool.globals["df_A"] = A
    return f"[anomaly_isoforest] cols={targets}, contamination={contamination_val}, outliers={int(flags.sum())}"

@tool
def cohort_compare(by: str = "model,fw", agg: str = "mean", cols: str = "") -> str:
    """
    Group df_A by categorical columns and aggregate numeric metrics.
    Saves to pytool.globals['df_cohort'].
    """
    A = pytool.globals.get("df_A")
    if A is None:
        return "df_A not loaded."
    keys = [k.strip() for k in (by or "").split(",") if k.strip()]
    for k in keys:
        if k not in A.columns:
            return f"Group key '{k}' not found."
    if cols:
        metrics = [c.strip() for c in cols.split(",") if c.strip()]
    else:
        metrics = [c for c in A.columns if pd.api.types.is_numeric_dtype(A[c])]
    if not metrics:
        return "No numeric metrics to aggregate."
    aggfn = {"mean":"mean","median":"median","max":"max","min":"min","count":"count"}.get(agg.lower())
    if aggfn is None:
        return "Unsupported agg. Use mean|median|max|min|count."
    g = A.groupby(keys, dropna=False)[metrics].agg(aggfn).reset_index()
    pytool.globals["df_cohort"] = g
    return f"[cohort_compare] by={keys}, agg={agg}, metrics={metrics}\nPreview:\n{g.head().to_markdown(index=False)}"

@tool
def topn_machines(metric: str = "uncorrectable_per_tb", n: Any = 20, machine_col: str = "machineID") -> str:
    """
    List top-N machines in df_A by a metric (descending).
    """
    A = pytool.globals.get("df_A")
    if A is None:
        return "df_A not loaded."
    n_val = _parse_int(n, 20)
    if machine_col not in A.columns:
        return f"machine_col '{machine_col}' not found."
    if metric not in A.columns:
        return f"metric '{metric}' not found."
    sub = A[[machine_col, metric]].copy()
    sub = sub.sort_values(metric, ascending=False).head(n_val)
    pytool.globals["df_topN"] = sub
    return f"[topn_machines] metric='{metric}', n={n_val}\n{sub.to_markdown(index=False)}"

# ---------- ì´ìƒì  ìë™í™” ë˜í¼ / ë³´ì¡° ìœ í‹¸ ----------
@tool
def select_numeric_candidates(min_unique: Any = 10, min_std: Any = 1e-9) -> str:
    """
    Return numeric candidate columns in df_A with >= min_unique and std > min_std.
    - min_unique: ë¹ˆ ë¬¸ìì—´/ë¬¸ìì—´ ìˆ«ì/ì •ìˆ˜ ëª¨ë‘ í—ˆìš©
    - min_std:    ë¹ˆ ë¬¸ìì—´/ë¬¸ìì—´ ìˆ«ì/ì‹¤ìˆ˜ ëª¨ë‘ í—ˆìš©
    """
    A = pytool.globals.get("df_A")
    if A is None:
        return "df_A not loaded."

    # âœ… ì•ˆì „ íŒŒì‹±
    min_unique_val = _parse_int(min_unique, 10)
    min_std_val = _parse_float(min_std, 1e-9)

    nums = []
    for c in A.columns:
        if pd.api.types.is_numeric_dtype(A[c]):
            u = A[c].nunique(dropna=True)
            s = pd.to_numeric(A[c], errors="coerce").std(skipna=True)
            if u >= min_unique_val and (s is not None and s > min_std_val and np.isfinite(s)):
                nums.append(c)
    if not nums:
        return "No numeric candidates."
    return "Numeric candidates:\n" + pd.DataFrame({"column": nums}).to_markdown(index=False)

@tool
def rank_outlier_columns(method: str = "iqr_ratio", top_n: Any = 20) -> str:
    """
    Rank numeric columns by outlier ratio (IQR). Returns a table (head top_n).
    """
    A = pytool.globals.get("df_A")
    if A is None:
        return "df_A not loaded."

    top_n_val = _parse_int(top_n, 20)

    num_cols = [c for c in A.columns if pd.api.types.is_numeric_dtype(A[c]) and A[c].nunique(dropna=True) >= 10]
    rows = []
    for c in num_cols:
        x = pd.to_numeric(A[c], errors="coerce")
        q1, q3 = x.quantile(0.25), x.quantile(0.75)
        iqr = q3 - q1
        if not np.isfinite(iqr) or iqr == 0:
            continue
        lo, hi = q1 - 1.5 * iqr, q3 + 1.5 * iqr
        rate = float(((x < lo) | (x > hi)).mean() * 100.0)
        rows.append({"column": c, "outlier_rate_%": rate, "lo": float(lo), "hi": float(hi)})
    if not rows:
        return "No IQR-detectable outliers."
    rank_df = pd.DataFrame(rows).sort_values("outlier_rate_%", ascending=False).head(top_n_val)
    pytool.globals["df_outlier_rank"] = rank_df
    return rank_df.to_markdown(index=False)

@tool
def plot_outliers(col: str, on: str = "datetime", sample: Any = 2000) -> str:
    """
    Plot boxplot and time series with IQR outlier highlighting for df_A[col].
    """
    A = pytool.globals.get("df_A")
    if A is None:
        return "df_A not loaded."
    sample_val = _parse_int(sample, 2000)
    s = str(col).strip()
    if s not in A.columns:
        return f"Column '{s}' not found."
    x = pd.to_numeric(A[s], errors="coerce")
    q1, q3 = x.quantile(0.25), x.quantile(0.75)
    iqr = q3 - q1
    if not np.isfinite(iqr) or iqr == 0:
        return "IQR is zero or invalid."
    lo, hi = q1 - 1.5 * iqr, q3 + 1.5 * iqr
    flags = (x < lo) | (x > hi)

    # Boxplot
    plt.figure()
    pd.DataFrame({s: x}).plot(kind="box")
    plt.title(f"Boxplot: {s} (IQR bounds: {lo:.2f}, {hi:.2f})")
    plt.xlabel("")
    plt.tight_layout()

    # Timeseries (if available)
    if on in A.columns:
        ts = pd.to_datetime(A[on], errors="coerce")
        dfv = pd.DataFrame({on: ts, s: x, "__out": flags}).dropna().sort_values(on)
        if sample_val and len(dfv) > sample_val:
            step = max(1, len(dfv)//sample_val)
            dfv = dfv.iloc[::step, :]
        plt.figure()
        plt.plot(dfv[on], dfv[s], linestyle="-", marker="", alpha=0.7)
        ou = dfv[dfv["__out"]]
        if len(ou) > 0:
            plt.scatter(ou[on], ou[s], marker="o", s=12)
        plt.title(f"Timeseries: {s} (outliers highlighted)")
        plt.tight_layout()

    return f"[plot_outliers] col='{s}', outliers={int(flags.sum())}, bounds=({lo:.3f},{hi:.3f})"

@tool
def auto_outlier_eda(top_n: Any = 10, on: str = "datetime") -> str:
    """
    Run an outlier-first EDA pipeline on df_A:
    - select numeric candidates
    - anomaly_iqr on each â†’ rank by outlier ratio
    - optional stl_decompose on top-1 if time column exists
    - optional anomaly_isoforest on k-best (if sklearn installed)
    Returns a concise ranked report (head 20).
    """
    A = pytool.globals.get("df_A")
    if A is None:
        return "df_A not loaded."

    top_n_val = _parse_int(top_n, 10)

    # 1) í›„ë³´ ì»¬ëŸ¼
    num_cols = [c for c in A.columns if pd.api.types.is_numeric_dtype(A[c])]
    num_cols = [c for c in num_cols if A[c].nunique(dropna=True) >= 10]
    if not num_cols:
        return "No numeric candidates."

    # 2) IQR ìŠ¤ìº”
    rows = []
    for c in num_cols:
        x = pd.to_numeric(A[c], errors="coerce")
        q1, q3 = x.quantile(0.25), x.quantile(0.75)
        iqr = q3 - q1
        if not np.isfinite(iqr) or iqr == 0:
            continue
        lo, hi = q1 - 1.5 * iqr, q3 + 1.5 * iqr
        flags = (x < lo) | (x > hi)
        rate = float(flags.mean() * 100.0)
        rows.append({"column": c, "outlier_rate_%": rate, "lo": float(lo), "hi": float(hi)})
    if not rows:
        return "No IQR-detectable outliers."
    rank_df = pd.DataFrame(rows).sort_values("outlier_rate_%", ascending=False)
    top_cols = rank_df.head(max(1, min(top_n_val, len(rank_df))))["column"].tolist()
    pytool.globals["df_outlier_rank"] = rank_df

    summary = ["[auto_outlier_eda] IQR scan complete.",
               "**Top outlier columns:**\n" + rank_df.head(20).to_markdown(index=False)]

    # 3) STL (ì„ íƒ)
    if on in A.columns and len(top_cols) > 0 and STL is not None:
        try:
            col = top_cols[0]
            ts = pd.to_datetime(A[on], errors="coerce")
            y = pd.to_numeric(A[col], errors="coerce")
            ok = ts.notna() & y.notna()
            ts, y = ts[ok], y[ok]
            y = pd.Series(y.values, index=ts).sort_index()
            if len(y) >= 48:
                res = STL(y, period=24, robust=True).fit()
                resid = pd.Series(res.resid, index=y.index)
                max_abs = float(resid.abs().max())
                when = str(resid.abs().idxmax())
                summary.append(f"**STL residual spike** for '{col}': max |resid|={max_abs:.3f} at {when}")
        except Exception:
            pass

    # 4) ë‹¤ë³€ëŸ‰ (ì„ íƒ)
    if IsolationForest is not None:
        try:
            k = min(4, len(top_cols))
            if k >= 2:
                X = A[top_cols[:k]].apply(pd.to_numeric, errors="coerce").dropna()
                if len(X) >= 50:
                    clf = IsolationForest(n_estimators=200, contamination=0.02, random_state=42).fit(X.values)
                    out_rate = float((clf.predict(X.values) == -1).mean() * 100.0)
                    summary.append(f"**IsolationForest** on {top_cols[:k]} â†’ outlier_rateâ‰ˆ{out_rate:.1f}%")
        except Exception:
            pass

    return "\n\n".join(summary)

# íˆ´ ëª©ë¡
tools = [
    pytool,
    load_loading_csv,
    describe_columns,
    save_plots_zip,
    load_df_b,
    describe_columns_on,
    sql_on_dfs,
    propose_join_keys,
    align_time_buckets,
    compare_on_keys,
    mismatch_report,
    make_timesafe,
    create_features,
    rolling_stats,
    stl_decompose,
    anomaly_iqr,
    anomaly_isoforest,
    cohort_compare,
    topn_machines,
    select_numeric_candidates,
    rank_outlier_columns,
    plot_outliers,
    auto_outlier_eda,
]

# =============================
# ğŸ¤– LLM ì„¤ì •
# =============================
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
if not GOOGLE_API_KEY:
    st.error("GOOGLE_API_KEYê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. .env ë˜ëŠ” í™˜ê²½ë³€ìˆ˜ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
    st.stop()

llm = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash",
    api_key=GOOGLE_API_KEY,
    temperature=0,
    max_tokens=None,
    timeout=None,
    max_retries=2,
    disable_streaming=False,  # streaming ê²½ê³  íšŒí”¼
)

# =============================
# ğŸ“œ Prompt (ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì—…ë°ì´íŠ¸)
# =============================
headA = df_A.head().to_string(index=False)
headB = df_B.head().to_string(index=False) if df_B is not None else "(df_B not loaded)"

react_prompt = ChatPromptTemplate.from_messages([
    ("system",
     "You are an expert data analyst for SSD telemetry and tabular data. "
     "You work with two dataframes: df_A (main, alias: df) and df_B (optional, for comparison/labels).\n\n"
     "When the user asks for outlier-focused EDA, DO NOT ask questions. Immediately run this pipeline:\n"
     "  1) make_timesafe(column='datetime', tz='UTC') if 'datetime' exists\n"
     "  2) describe_columns â†’ select_numeric_candidates â†’ rank_outlier_columns\n"
     "  3) anomaly_iqr on top-N columns\n"
     "  4) stl_decompose on top-1 if time series available\n"
     "  5) anomaly_isoforest on k-best numeric (if sklearn available)\n"
     "  6) cohort_compare(by='model,fw', agg='mean') if columns exist\n"
     "  7) Summarize: top outlier columns + time spikes + cohort outliers + next steps.\n"
     "If df_B is loaded and user mentions comparison, use propose_join_keys then compare_on_keys.\n\n"
     "ALWAYS follow this EXACT format:\n"
     "Question: <restated question>\n"
     "Thought: <brief reasoning>\n"
     "Action: <ONE tool name from {tool_names}>\n"
     "Action Input: <valid input with NO backticks>\n"
     "Observation: <tool result>\n"
     "(Repeat Thought/Action/Action Input/Observation as needed)\n"
     "Thought: I now know the final answer\n"
     "Final Answer: <concise answer>\n\n"
     "If you output anything outside this format, continue immediately by outputting ONLY a valid 'Action' and 'Action Input'.\n\n"
     "Tool routing guide:\n"
     "- Schema/summary â†’ describe_columns, describe_columns_on\n"
     "- File load â†’ load_loading_csv, load_df_b\n"
     "- SQL/join/aggregation â†’ sql_on_dfs\n"
     "- TWO-CSV comparison â†’ propose_join_keys â†’ compare_on_keys('machineID,datetime') â†’ mismatch_report('...')\n"
     "- SSD utilities â†’ make_timesafe, create_features, rolling_stats, stl_decompose, anomaly_iqr, anomaly_isoforest, cohort_compare, topn_machines\n"
     "- Outlier one-click â†’ auto_outlier_eda, then plot_outliers on top columns\n"
     "- Custom compute/plots â†’ python_repl_ast (do complex tasks in ONE call and print results)\n\n"
     "For tools that take a column name (e.g., anomaly_iqr), pass ONLY the raw column name (e.g., temperature), "
     "not 'column=temperature'. If you accidentally wrote 'column=...', immediately continue by outputting just the raw name.\n\n"
     "Call compare_on_keys with just the keys string (e.g., 'machineID,datetime') or as JSON {{\"keys\":\"machineID,datetime\"}}.\n"
     "Do NOT pass \"keys='...'\" literal unless JSON.\n\n"
     f"df_A.head():\n{headA}\n\n"
     f"df_B.head():\n{headB}\n"
    ),
    MessagesPlaceholder("chat_history", optional=True),
    ("human", "{input}"),
    ("assistant", "{agent_scratchpad}"),
])

# {tools}/{tool_names} ì£¼ì…
tool_desc = render_text_description(tools)
tool_names = ", ".join([t.name for t in tools])
react_prompt = react_prompt.partial(tools=tool_desc, tool_names=tool_names)

# =============================
# âš™ï¸ ReAct Agent
# =============================
react_runnable = create_react_agent(llm, tools, prompt=react_prompt)
agent = AgentExecutor(
    agent=react_runnable,
    tools=tools,
    verbose=True,
    return_intermediate_steps=True,
    max_iterations=20,
    early_stopping_method="generate",
    handle_parsing_errors=(
        "PARSING ERROR. DO NOT APOLOGIZE. Immediately continue by outputting ONLY:\n"
        "Action: describe_columns\n"
        "Action Input: \n"
    ),
)

# =============================
# ğŸ”„ RunnableWithMessageHistory
# =============================
history = StreamlitChatMessageHistory(key="lc_msgs:dfchat")
agent_with_history = RunnableWithMessageHistory(
    agent,
    lambda session_id: history,
    input_messages_key="input",
    history_messages_key="chat_history"
)

# =============================
# ğŸ’» UI ì‹¤í–‰
# =============================
st.write("---")
user_q = st.chat_input(
    "ì˜ˆ) ì´ìƒì  EDA í•´ì¤˜ / auto_outlier_eda() / plot_outliers('temperature') / "
    "propose_join_keys / compare_on_keys('machineID,datetime') / mismatch_report('temperature') / "
    "rolling_stats(cols='temperature,uncorrectable_error_count', window='24H') / stl_decompose('temperature', 24)"
)

if user_q:
    left, right = st.columns([1, 1])
    with left:
        st.subheader("ì‹¤ì‹œê°„ ì‹¤í–‰ ë¡œê·¸")
        st_cb = StreamlitCallbackHandler(st.container())
    collector = SimpleCollectCallback()

    with st.spinner("Thinking with Gemini..."):
        try:
            result = agent_with_history.invoke(
                {"input": user_q},
                {
                    "callbacks": [st_cb, collector, StdOutCallbackHandler()],
                    "configurable": {"session_id": "two_csv_compare_and_ssd_eda"},
                }
            )
        except Exception as e:
            st.error(f"Agent ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
            result = {"output": f"Agent ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}"}

    st.success("Done.")
    final = result.get("output", "Agentê°€ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
    with right:
        st.subheader("Answer")
        st.write(final)

        # í˜„ì¬ê¹Œì§€ ìƒì„±ëœ matplotlib í”Œë¡¯ ë Œë”ë§
        figs = [plt.figure(n) for n in plt.get_fignums()]
        for f in figs:
            f.set_size_inches(6, 4)
            f.set_dpi(100)
            st.pyplot(f, width="stretch")
        plt.close("all")

    # intermediate steps (ë””ë²„ê¹… ì •ë³´)
    st.write("---")
    st.subheader("intermediate_steps (íˆ´ ì‹¤í–‰ ìƒì„¸)")
    steps = result.get("intermediate_steps", [])
    if steps:
        for i, (action, observation) in enumerate(steps, 1):
            with st.expander(f"Step {i}: {action.tool}"):
                st.markdown("**tool_input**")
                st.code(str(action.tool_input))
                st.markdown("**observation**")
                obs_txt = observation if isinstance(observation, str) else str(observation)
                st.code(obs_txt[:4000])
    else:
        st.info("intermediate_steps ë¹„ì–´ ìˆìŒ")

    # ì½œë°± íƒ€ì„ë¼ì¸ (ë””ë²„ê¹… ì •ë³´)
    st.write("---")
    st.subheader("ì½œë°± ì´ë²¤íŠ¸ íƒ€ì„ë¼ì¸ (Simple Collect)")
    if collector.events:
        for j, e in enumerate(collector.events, 1):
            with st.expander(f"Event {j}: {e.get('type')}"):
                if e.get("type") == "llm_start":
                    prompts = e.get("prompts", [])
                    for k, p in enumerate(prompts, 1):
                        st.markdown(f"**Prompt {k}**")
                        st.code(p)
                else:
                    st.write(e)
    else:
        st.info("ì½œë°± ì´ë²¤íŠ¸ ì •ë³´ ì—†ìŒ")

# =============================
# Streamlit ì±„íŒ… ê¸°ë¡ í‘œì‹œ
# =============================
st.sidebar.title("Chat History")
for msg in history.messages:
    if msg.type == "human":
        with st.sidebar:
            st.chat_message("user").write(msg.content)
    elif msg.type == "ai":
        with st.sidebar:
            st.chat_message("assistant").write(msg.content[:50] + "...")
