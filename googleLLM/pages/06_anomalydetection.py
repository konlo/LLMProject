import os
import sys
import io
import zipfile
import pandas as pd
import streamlit as st
from dotenv import load_dotenv
import matplotlib.pyplot as plt
import duckdb
from typing import Any, Dict, List

# LangChain ë° Gemini ë¼ì´ë¸ŒëŸ¬ë¦¬
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_community.callbacks.streamlit import StreamlitCallbackHandler
from langchain_community.chat_message_histories import StreamlitChatMessageHistory
from langchain_experimental.tools.python.tool import PythonAstREPLTool
from langchain.agents import create_react_agent, AgentExecutor
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.tools import tool
from langchain.callbacks import StdOutCallbackHandler
from langchain_core.callbacks.base import BaseCallbackHandler
import pandas as pd  
from sklearn.model_selection import train_test_split  
from sklearn.ensemble import RandomForestClassifier  
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix  
import matplotlib.pyplot as plt  
import seaborn as sns  


# =======================================================================
# ğŸ› ï¸ ì½œë°± í´ë˜ìŠ¤ ìˆ˜ì •: BaseCallbackHandler ìƒì† (ì˜¤ë¥˜ í•´ê²°)
# =======================================================================
class SimpleCollectCallback(BaseCallbackHandler):
    """
    LangChain AgentExecutorì™€ì˜ í˜¸í™˜ì„±ì„ ìœ„í•´ BaseCallbackHandlerë¥¼ ìƒì†ë°›ìŠµë‹ˆë‹¤.
    (ì›ë˜ì˜ CollectAllCallback ëª¨ë“ˆ ëŒ€ì²´ìš©)
    """
    def __init__(self):
        self.events = []
    
    def on_tool_error(self, error: Exception, **kwargs: Any) -> Any:
        # AgentExecutorê°€ ì˜¤ë¥˜ ë°œìƒ ì‹œ í˜¸ì¶œí•˜ëŠ” í•„ìˆ˜ ë©”ì„œë“œ
        self.events.append({"type": "tool_error", "error": str(error)})
        
    def on_llm_start(self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any) -> None:
        self.events.append({"type": "llm_start", "prompts": prompts})
        
    def on_llm_end(self, response: Any, **kwargs: Any) -> None:
        self.events.append({"type": "llm_end"})

    # AgentExecutor í˜¸í™˜ì„± ìœ ì§€ë¥¼ ìœ„í•´ í•„ìˆ˜ì ì¸ ìµœì†Œ ë©”ì„œë“œ ì •ì˜
    def on_agent_action(self, action: Any, **kwargs: Any) -> Any:
        pass
    
    def on_agent_finish(self, finish: Any, **kwargs: Any) -> Any:
        pass

# =============================
# ğŸš€ App ì‹œì‘ ë° ì„¤ì •
# =============================
load_dotenv()

st.set_page_config(page_title="DF Chatbot (Gemini)", page_icon="âœ¨", layout="wide")
st.title("âœ¨ DataFrame Chatbot (Gemini + LangChain)")
st.caption("Gemini + Python tool(ReAct)ë¡œ DataFrameì„ ë¶„ì„í•˜ê³  ì´ìƒì ì„ ê²€í† í•©ë‹ˆë‹¤.")

# =============================
# ğŸ“ CSV ë¡œë“œ ë° í™˜ê²½ ë³€ìˆ˜
# =============================
# âš ï¸ DATA_DIR ì´ˆê¸°í™”
DEFAULT_DATA_DIR = os.getenv("DATA_DIR", "/Users/najongseong/dataset")
DFB_DEFAULT_NAME = "telemetry_raw.csv" # df_B ê¸°ë³¸ íŒŒì¼ëª…

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” ë° DATA_DIR ì„¤ì •
if "DATA_DIR" not in st.session_state:
    st.session_state["DATA_DIR"] = DEFAULT_DATA_DIR
if "df_A_data" not in st.session_state:
    st.session_state["df_A_data"] = None
if "df_A_name" not in st.session_state:
    st.session_state["df_A_name"] = "No Data"
if "csv_path" not in st.session_state:
    # ì´ˆê¸° csv_pathëŠ” ê¸°ë³¸ ë””ë ‰í† ë¦¬ì˜ ê¸°ë³¸ íŒŒì¼ë¡œ ì„¤ì • (ë¡œë“œê°€ ë  ê²½ìš°)
    st.session_state["csv_path"] = os.path.join(DEFAULT_DATA_DIR, "stormtrooper.csv") 

# í˜„ì¬ ì‚¬ìš© DATA_DIR
DATA_DIR = st.session_state["DATA_DIR"]
DFB_DEFAULT = os.path.join(DATA_DIR, DFB_DEFAULT_NAME)

# -----------------------------------------------------------------------
# ğŸ”„ CSV íŒŒì¼ ëª©ë¡ í‘œì‹œ ë° ì„ íƒ ë¡œì§
# -----------------------------------------------------------------------

def load_df_A(path: str, display_name: str):
    """ì§€ì •ëœ ê²½ë¡œì—ì„œ df_Aë¥¼ ë¡œë“œí•˜ê³  ì„¸ì…˜ ìƒíƒœë¥¼ ì—…ë°ì´íŠ¸í•˜ëŠ” í—¬í¼ í•¨ìˆ˜"""
    try:
        new_df = pd.read_csv(path)
        st.session_state["df_A_data"] = new_df
        st.session_state["df_A_name"] = display_name
        # âœ… ì„ íƒí•œ íŒŒì¼ì˜ ì „ì²´ ê²½ë¡œë¥¼ csv_pathì— ì •í™•íˆ ì—°ê²°
        st.session_state["csv_path"] = path 
        return True, f"Loaded file: {display_name} (Shape: {new_df.shape})"
    except Exception as e:
        st.session_state["df_A_data"] = None
        st.session_state["df_A_name"] = "Load Failed"
        # ë¡œë“œ ì‹¤íŒ¨ ì‹œ ê²½ë¡œë„ ì´ˆê¸°í™”
        st.session_state["csv_path"] = "" 
        return False, f"CSV ë¡œë“œ ì‹¤íŒ¨: {path}\n{e}"

with st.sidebar:
    st.markdown("### ğŸ—‚ï¸ 1. ë°ì´í„° í´ë” ì„¤ì •")
    
    # DATA_DIR ì…ë ¥ í•„ë“œ
    new_data_dir = st.text_input(
        "Enter CSV Directory Path",
        value=st.session_state["DATA_DIR"],
        key="data_dir_input"
    )

    if st.button("Set Directory"):
        if os.path.isdir(new_data_dir):
            st.session_state["DATA_DIR"] = new_data_dir
            st.session_state["df_A_data"] = None # í´ë” ë³€ê²½ ì‹œ ë°ì´í„° ì´ˆê¸°í™”
            st.session_state["df_A_name"] = "No Data"
            # âœ… í´ë” ë³€ê²½ ì‹œ csv_pathë„ ì´ˆê¸°í™”í•˜ì—¬ íŒŒì¼ ëª©ë¡ì„ ìƒˆë¡œ ê³ ì¹˜ë„ë¡ ìœ ë„
            st.session_state["csv_path"] = "" 
            st.success(f"Directory set to: `{new_data_dir}`")
            # ë””ë ‰í† ë¦¬ ë³€ê²½ í›„ ì¬ì‹¤í–‰í•˜ì—¬ íŒŒì¼ ëª©ë¡ì„ ì—…ë°ì´íŠ¸
            st.rerun() 
        else:
            st.error(f"Invalid directory path: `{new_data_dir}`")

    # ê°±ì‹ ëœ DATA_DIR
    DATA_DIR = st.session_state["DATA_DIR"]
    DFB_DEFAULT = os.path.join(DATA_DIR, DFB_DEFAULT_NAME)

    st.markdown("---")
    st.markdown("### ğŸ“„ 2. df_A CSV íŒŒì¼ ì„ íƒ")
    st.caption(f"Search directory: `{DATA_DIR}`")
    
    # DATA_DIRì—ì„œ CSV íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    csv_files = []
    try:
        if os.path.isdir(DATA_DIR):
            for f in os.listdir(DATA_DIR):
                if f.lower().endswith('.csv'):
                    csv_files.append(f)
            csv_files.sort()
        else:
            st.warning("ìœ íš¨í•œ ë°ì´í„° ë””ë ‰í† ë¦¬ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
    except Exception as e:
        st.error(f"í´ë” ì ‘ê·¼ ì˜¤ë¥˜: {e}")

    # íŒŒì¼ ì„ íƒ SelectBox
    selected_file = st.selectbox(
        "Select CSV file for df_A",
        options=["--- Select a file ---"] + csv_files,
        key="file_selector"
    )
    
    # íŒŒì¼ ë¡œë“œ ë²„íŠ¼
    if st.button("Load Selected File"):
        if selected_file and selected_file != "--- Select a file ---":
            file_path = os.path.join(DATA_DIR, selected_file)
            # ì´ ì‹œì ì—ì„œ load_df_Aê°€ st.session_state["csv_path"]ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
            success, message = load_df_A(file_path, selected_file) 
            if success:
                st.success(message)
                st.rerun() # íŒŒì¼ ë¡œë“œ í›„ UI ì—…ë°ì´íŠ¸ë¥¼ ìœ„í•´ ì¬ì‹¤í–‰
            else:
                st.error(message)
        else:
            st.warning("íŒŒì¼ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")

    st.markdown("---")
    # í˜„ì¬ ë¡œë“œëœ íŒŒì¼ ê²½ë¡œ í‘œì‹œ (ë””ë²„ê¹…ìš©)
    st.caption(f"**í˜„ì¬ ë¡œë“œ íŒŒì¼ ê²½ë¡œ:** `{st.session_state.get('csv_path', 'Not loaded')}`")
    st.caption(f"df_B ê¸°ë³¸ ê°€ì • íŒŒì¼: `{os.path.basename(DFB_DEFAULT)}`")

# ìµœì¢… df ê²°ì •
df = st.session_state["df_A_data"]
CSV_PATH_DISPLAY = st.session_state["df_A_name"]

if df is None:
    st.error("ë¶„ì„í•  DataFrame (df_A)ì„ ë¡œë“œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ìœ íš¨í•œ ë””ë ‰í† ë¦¬ì™€ CSV íŒŒì¼ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")
    st.stop()
# -----------------------------------------------------------------------
# ì´í•˜ LLM ë° Agent ë¡œì§ì€ ë³€ê²½ ì—†ì´ ìœ ì§€ë©ë‹ˆë‹¤.
# -----------------------------------------------------------------------

# í™˜ê²½ ë³€ìˆ˜ í™•ì¸
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
if not GOOGLE_API_KEY:
    st.error("GOOGLE_API_KEYê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. .env ë˜ëŠ” í™˜ê²½ë³€ìˆ˜ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
    st.stop()

# =============================
# ğŸ¤– LLM ì„¸íŒ…
# =============================
llm = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash",
    api_key=GOOGLE_API_KEY,
    temperature=0,
    max_tokens=None,
    timeout=None,
    max_retries=2,
    streaming=True,
)

# =============================
# ğŸ–¼ï¸ Preview
# =============================
st.subheader("Preview")
st.write(f"**Loaded CSV for df_A:** `{CSV_PATH_DISPLAY}` (Shape: {df.shape})")
st.dataframe(df.head(10), use_container_width=True)

# =============================
# ğŸ’¬ Chat history
# =============================
history = StreamlitChatMessageHistory(key="lc_msgs:dfchat")

# =============================
# ğŸ› ï¸ Tools ì •ì˜ ë° Agent Globals (DATA_DIR ë° df ë³€ìˆ˜ ì—…ë°ì´íŠ¸)
# =============================

# 1) Python ì‹¤í–‰ íˆ´: df_A/df_B/df/loading_df ë…¸ì¶œ
pytool = PythonAstREPLTool(
    globals={
        "pd": pd,
        "plt": plt,
        "df": df,      # í˜¸í™˜ì„± ìœ ì§€ (df == df_A)
        "df_A": df,    # ë©”ì¸ ë°ì´í„°ì…‹
        "df_B": None,  # ë³´ì¡° ë°ì´í„°ì…‹ (íˆ´ë¡œ ë¡œë“œ)
        "loading_df": None,  # ad-hoc ë¶„ì„ìš©
    },
    name="python_repl_ast",
    description=(
        "Execute Python on df_A/df_B with pandas/matplotlib. 'df' aliases df_A. "
        "Use this for custom computation, plotting (e.g., boxplot for outliers), and advanced data manipulation."
    )
)

# 2) ë™ì  CSV ë¡œë“œ (loading_df)
@tool
def load_loading_csv(filename: str) -> str:
    """Load a CSV from DATA_DIR into 'loading_df' (for ad-hoc analysis). Pass only file name (e.g., 'loading_test.csv')."""
    # íˆ´ ì‹¤í–‰ ì‹œ í˜„ì¬ DATA_DIRì„ ì‚¬ìš©
    current_data_dir = st.session_state.get("DATA_DIR", DEFAULT_DATA_DIR)
    path = os.path.join(current_data_dir, filename)
    try:
        new_df = pd.read_csv(path)
    except Exception as e:
        return f"Failed to load {path}: {e}"
    pytool.globals["loading_df"] = new_df
    preview = new_df.head(10).to_markdown(index=False)
    shape = f"{new_df.shape[0]} rows x {new_df.shape[1]} cols"
    return f"Loaded {filename} from {current_data_dir} into loading_df\nShape: {shape}\n\nPreview (head):\n{preview}"

# 3) ì»¬ëŸ¼ ìš”ì•½ (loading_df ìš°ì„  â†’ ì—†ìœ¼ë©´ df_A)
@tool
def describe_columns(cols: str = "") -> str:
    """
    Describe selected columns (comma-separated).
    Uses 'loading_df' if available; otherwise uses 'df_A'.
    This is useful for initial data quality checks and outlier boundary overview.
    """
    if "loading_df" in pytool.globals and pytool.globals["loading_df"] is not None:
        current_df = pytool.globals["loading_df"]
        source = "loading_df"
    else:
        current_df = pytool.globals.get("df_A", df) if pytool.globals.get("df_A") is not None else df
        source = "df_A"

    use_cols = [c.strip() for c in cols.split(",") if c.strip()] or list(current_df.columns)
    missing = [c for c in use_cols if c not in current_df.columns]
    if missing:
        return f"Missing columns: {missing}\nAvailable: {list(current_df.columns)}"

    desc = current_df[use_cols].describe(include="all").transpose()
    shape = f"{current_df.shape[0]} rows x {current_df.shape[1]} cols"
    return f"[source={source} | shape={shape}]\n\n" + desc.to_markdown()

# 4) í”Œë¡¯ ZIP
@tool
def save_plots_zip() -> str:
    """Zip all current matplotlib figures. Use after plotting with python_repl_ast."""
    figs = [plt.figure(n) for n in plt.get_fignums()]
    if not figs:
        return "No figures to save."
    buf = io.BytesIO()
    with zipfile.ZipFile(buf, "w", zipfile.ZIP_DEFLATED) as zf:
        for i, f in enumerate(figs, 1):
            img = io.BytesIO()
            f.savefig(img, format="png", dpi=100, bbox_inches="tight")
            img.seek(0)
            zf.writestr(f"plot_{i}.png", img.read())
    return f"Zipped {len(figs)} plots. Bytes={len(buf.getvalue())}"

# 5) df_B ë¡œë”
@tool
def load_df_b(filename: str = "") -> str:
    """
    Load a CSV into 'df_B'. If 'filename' is empty, defaults to 'telemetry_raw.csv' under DATA_DIR.
    Example: load_df_b()
    """
    current_data_dir = st.session_state.get("DATA_DIR", DEFAULT_DATA_DIR)
    
    if not filename:
        path = os.path.join(current_data_dir, DFB_DEFAULT_NAME)
        show_name = os.path.basename(path)
    else:
        path = os.path.join(current_data_dir, filename) if not os.path.isabs(filename) else filename
        show_name = os.path.basename(path)

    try:
        new_df = pd.read_csv(path)
    except Exception as e:
        return f"Failed to load df_B from {path}: {e}"
    pytool.globals["df_B"] = new_df
    preview = new_df.head(10).to_markdown(index=False)
    return f"Loaded df_B from '{show_name}' (full: {path}) with shape {new_df.shape}\n\nPreview:\n{preview}"

# 6) df_A / df_B ëŒ€ìƒ ì§€ì • ìš”ì•½
@tool
def describe_columns_on(target: str = "A", cols: str = "") -> str:
    """
    Describe columns from df_A or df_B.
    target: 'A' or 'B'
    cols: comma-separated; empty -> all
    """
    t = (target or "A").strip().upper()
    if t == "B":
        if "df_B" not in pytool.globals or pytool.globals["df_B"] is None:
            return "df_B is not loaded. Use load_df_b() first."
        current_df = pytool.globals["df_B"]
        source = "df_B"
    else:
        current_df = pytool.globals.get("df_A", df) if pytool.globals.get("df_A") is not None else df
        source = "df_A"

    use_cols = [c.strip() for c in cols.split(",") if c.strip()] or list(current_df.columns)
    missing = [c for c in use_cols if c not in current_df.columns]
    if missing:
        return f"Missing columns: {missing}\nAvailable: {list(current_df.columns)}"

    desc = current_df[use_cols].describe(include="all").transpose()
    shape = f"{current_df.shape[0]} rows x {current_df.shape[1]} cols"
    return f"[source={source} | shape={shape}]\n\n" + desc.to_markdown()

# 7) df_A/df_B SQL ì§ˆì˜ (DuckDB)
@tool
def sql_on_dfs(query: str) -> str:
    """
    Run DuckDB SQL over df_A and (if loaded) df_B. Tables: df_A, df_B
    Example: SELECT a.key, a.metric FROM df_A a WHERE a.metric > 100 LIMIT 20
    """
    try:
        duckdb.register("df_A", pytool.globals.get("df_A", df) if pytool.globals.get("df_A") is not None else df)
        if pytool.globals.get("df_B") is not None:
            duckdb.register("df_B", pytool.globals["df_B"])
        out = duckdb.sql(query).df()
        return out.head(200).to_markdown(index=False)
    except Exception as e:
        return f"SQL error: {e}"

# 8) ì¡°ì¸ í‚¤ í›„ë³´ ì¶”ì²œ
@tool
def propose_join_keys() -> str:
    """Suggest join key candidates between df_A and df_B by intersecting column names and compatible dtypes."""
    A = pytool.globals.get("df_A") if pytool.globals.get("df_A") is not None else df
    B = pytool.globals.get("df_B")
    if B is None:
        return "df_B is not loaded."

    def dtype_sig(series):
        t = str(series.dtype)
        if "int" in t: return "int"
        if "float" in t: return "float"
        if "datetime" in t or "date" in t or "time" in t: return "datetime"
        return "str"

    pairs = []
    for c in A.columns:
        if c in B.columns and dtype_sig(A[c]) == dtype_sig(B[c]):
            pairs.append((c, dtype_sig(A[c])))

    if not pairs:
        return "No obvious same-name & same-type keys. Consider mapping tables or casting."

    md = "| key | dtype |\n|---|---|\n" + "\n".join([f"| {k} | {t} |" for k, t in pairs])
    return f"Candidate join keys (same name & type):\n{md}"

# 9) íƒ€ì„ë²„í‚· ì •ë ¬
@tool
def align_time_buckets(target: str = "A", column: str = "ts", freq: str = "H") -> str:
    """
    Resample time-like column to buckets and store as df_A_bucketed or df_B_bucketed.
    target: 'A' or 'B'; column must be timestamp-like; freq like 'H','D','15min'
    """
    t = (target or "A").strip().upper()
    cur = None
    if t == "B":
        cur = pytool.globals.get("df_B")
        if cur is None:
            return "df_B is not loaded."
    else:
        cur = pytool.globals.get("df_A") if pytool.globals.get("df_A") is not None else df

    if column not in cur.columns:
        return f"Column '{column}' not in df_{t}. Available: {list(cur.columns)}"

    tmp = cur.copy()
    tmp[column] = pd.to_datetime(tmp[column], errors="coerce")
    bucket_col = f"{column}_bucket"
    tmp[bucket_col] = tmp[column].dt.to_period(freq).dt.to_timestamp()

    key = f"df_{t}_bucketed"
    pytool.globals[key] = tmp
    prev = tmp[[bucket_col]].head().to_markdown(index=False)
    return f"Created {key} with '{bucket_col}' at freq={freq}.\nPreview:\n{prev}"

tools = [
    pytool,
    load_loading_csv,
    describe_columns,
    save_plots_zip,
    load_df_b,
    describe_columns_on,
    sql_on_dfs,
    propose_join_keys,
    align_time_buckets,
]

# =============================
# ğŸ“œ Prompt (ì´ìƒì  ê²€í†  ê°•í™”)
# =============================
df_head_txt = df.head().to_string(index=False)



import os
# ... (ìƒëµëœ importë¬¸) ...
from langchain_core.callbacks.base import BaseCallbackHandler
import pandas as pd  
from sklearn.model_selection import train_test_split  
from sklearn.ensemble import RandomForestClassifier  
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix  
import matplotlib.pyplot as plt  
import seaborn as sns  


# =======================================================================
# ğŸ› ï¸ ì½œë°± í´ë˜ìŠ¤ ìˆ˜ì •: BaseCallbackHandler ìƒì† (ì˜¤ë¥˜ í•´ê²°)
# ... (SimpleCollectCallback í´ë˜ìŠ¤ ì •ì˜ ìƒëµ) ...
# =============================
# ğŸš€ App ì‹œì‘ ë° ì„¤ì •
# ... (App ì‹œì‘ ë° ì„¤ì • ì½”ë“œ ìƒëµ) ...
# =============================
# ğŸ“ CSV ë¡œë“œ ë° í™˜ê²½ ë³€ìˆ˜
# ... (CSV ë¡œë“œ ë° í™˜ê²½ ë³€ìˆ˜ ì½”ë“œ ìƒëµ) ...
# -----------------------------------------------------------------------
# ğŸ”„ CSV íŒŒì¼ ëª©ë¡ í‘œì‹œ ë° ì„ íƒ ë¡œì§
# ... (CSV ë¡œë“œ ë° ì„ íƒ ë¡œì§ ì½”ë“œ ìƒëµ) ...
# -----------------------------------------------------------------------
# ì´í•˜ LLM ë° Agent ë¡œì§ì€ ë³€ê²½ ì—†ì´ ìœ ì§€ë©ë‹ˆë‹¤.
# -----------------------------------------------------------------------

# í™˜ê²½ ë³€ìˆ˜ í™•ì¸
# ... (í™˜ê²½ ë³€ìˆ˜ í™•ì¸ ì½”ë“œ ìƒëµ) ...

# =============================
# ğŸ¤– LLM ì„¸íŒ…
# ... (LLM ì„¸íŒ… ì½”ë“œ ìƒëµ) ...
# =============================
# ğŸ–¼ï¸ Preview
# ... (Preview ì½”ë“œ ìƒëµ) ...
# =============================
# ğŸ’¬ Chat history
# ... (Chat history ì½”ë“œ ìƒëµ) ...
# =============================
# ğŸ› ï¸ Tools ì •ì˜ ë° Agent Globals (DATA_DIR ë° df ë³€ìˆ˜ ì—…ë°ì´íŠ¸)
# ... (Tools ì •ì˜ ì½”ë“œ ìƒëµ) ...
# 9) íƒ€ì„ë²„í‚· ì •ë ¬
# ... (align_time_buckets ë„êµ¬ ì •ì˜ ìƒëµ) ...

tools = [
    pytool,
    load_loading_csv,
    describe_columns,
    save_plots_zip,
    load_df_b,
    describe_columns_on,
    sql_on_dfs,
    propose_join_keys,
    align_time_buckets,
]

# =============================
# ğŸ“œ Prompt (ì´ìƒì  ê²€í†  ê°•í™”)  <-- ğŸ¯ ì´ ì„¹ì…˜ì´ ìˆ˜ì •ë˜ì—ˆìŠµë‹ˆë‹¤.
# =============================
df_head_txt = df.head().to_string(index=False)

react_prompt = ChatPromptTemplate.from_messages([
    ("system",
     "You are working with pandas dataframes. The main dataframe is `df_A` (alias: df). "
     "You may also load a related dataframe as `df_B` for root-cause analysis.\n\n"
     "Tools available:\n{tools}\nUse only tool names from: {tool_names}.\n"
     "If you use sql_on_dfs, available tables are df_A and (if loaded) df_B.\n\n"
     "Tool routing guide:\n"
     "- schema/summary/initial outlier bounds â†’ describe_columns or describe_columns_on\n"
     "- load file â†’ load_loading_csv or load_df_b\n"
     "- SQL/join/aggregation â†’ sql_on_dfs\n"
     "- **custom compute/plots/outlier analysis** â†’ **python_repl_ast**\n"
     "- suggest join keys â†’ propose_join_keys\n"
     "- align timestamps to buckets â†’ align_time_buckets\n\n"
     "**âš ï¸ ì´ìƒì (Outlier) ê²€í†  ì§€ì¹¨:**\n"
     "1. **describe_columns**ë¥¼ ì‚¬ìš©í•˜ì—¬ ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ì˜ min/max/std/ì‚¬ë¶„ìœ„ìˆ˜(Q1, Q3)ë¥¼ í™•ì¸í•˜ì„¸ìš”.\n"
     "2. **python_repl_ast**ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°•ìŠ¤ í”Œë¡¯(Box Plot)ì„ ê·¸ë ¤ ì‹œê°ì ìœ¼ë¡œ ì´ìƒì ì„ í™•ì¸í•˜ì„¸ìš”.\n"
     "   - ì˜ˆì‹œ: `df['value_col'].plot(kind='box'); plt.title('Box Plot'); plt.show()`\n"
     "3. **python_repl_ast**ë¥¼ ì‚¬ìš©í•˜ì—¬ IQR(Q3 + 1.5*IQR) ë“±ì˜ í†µê³„ì  ê²½ê³„ ì¡°ê±´ì„ ê³„ì‚°í•˜ê³ , ì´ìƒì ì„ í•„í„°ë§í•˜ì—¬ í™•ì¸í•˜ì„¸ìš”.\n\n"
     # âœ… ë³€ìˆ˜ ì§€ì†ì„± ì˜¤ë¥˜ ë°©ì§€ë¥¼ ìœ„í•œ ìƒˆ ì§€ì¹¨ ì¶”ê°€
     "**ğŸš¨ Python ì‹¤í–‰ í•„ìˆ˜ ì§€ì¹¨ (ë³€ìˆ˜ ì§€ì†ì„± ì˜¤ë¥˜ ë°©ì§€):**\n"
     "1. ë³µì¡í•œ ì‘ì—…(ë°ì´í„° ì¤€ë¹„, ëª¨ë¸ ì •ì˜, í•™ìŠµ, ì˜ˆì¸¡ ë“±)ì€ ë³€ìˆ˜ ì§€ì†ì„±ì„ ë³´ì¥í•˜ê¸° ìœ„í•´ **ë°˜ë“œì‹œ í•˜ë‚˜ì˜ `python_repl_ast` íˆ´ í˜¸ì¶œ ë‚´**ì—ì„œ ì‹¤í–‰í•˜ì‹­ì‹œì˜¤.\n"
     "2. ì½”ë“œë¥¼ ì—¬ëŸ¬ 'Action:'ìœ¼ë¡œ ë¶„í• í•˜ì§€ ë§ˆì‹­ì‹œì˜¤. ë³€ìˆ˜(`model`, `X_train`, `results` ë“±)ê°€ ë‹¤ìŒ ë‹¨ê³„ì—ì„œ ìœ ì‹¤ë˜ì–´ ì˜¤ë¥˜ê°€ ë°œìƒí•©ë‹ˆë‹¤.\n"
     "3. ìµœì¢… ê²°ê³¼ë¥¼ ì‚¬ìš©ìì—ê²Œ ë³´ì—¬ì¤„ ë•ŒëŠ” ì½”ë“œ ë¸”ë¡ì˜ ë§ˆì§€ë§‰ì— `print(ê²°ê³¼ë³€ìˆ˜)` ëª…ë ¹ì„ í¬í•¨í•˜ì—¬ **Observation**ìœ¼ë¡œ ê²°ê³¼ë¥¼ ëª…í™•íˆ ë°˜í™˜í•´ì•¼ í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ìµœì¢…ì ìœ¼ë¡œ í™•ì¸í•˜ëŠ” ê²½ìš° `print(results)` ë˜ëŠ” `print(accuracy)`ë¥¼ ì‚¬ìš©í•˜ì‹­ì‹œì˜¤.\n\n"
     # -------------------------------------------------------------
     "Parsing recovery:\n"
     "- If you output 'Action:' without 'Action Input:', immediately continue with only 'Action Input: <...>'.\n"
     "- Do not wrap code in backticks.\n"
     "- Keep Action Input minimal, valid, and executable.\n\n"
     "ALWAYS follow this exact format:\n"
     "Question: <restated question>\n"
     "Thought: <brief reasoning>\n"
     "Action: <ONE tool name from {tool_names}>\n"
     "Action Input: <valid code with NO backticks>\n"
     "Observation: <tool result>\n"
     "(Repeat Thought/Action/Action Input/Observation as needed)\n"
     "Thought: I now know the final answer\n"
     "Final Answer: <answer>\n\n"
     f"This is the result of print(df_A.head()):\n{df_head_txt}\n"),
    MessagesPlaceholder("chat_history", optional=True),
    ("human", "{input}"),
    ("assistant", "{agent_scratchpad}"),
])

# =============================
# âš™ï¸ ReAct Agent
# =============================
react_runnable = create_react_agent(llm, tools, prompt=react_prompt)
agent = AgentExecutor(
    agent=react_runnable,
    tools=tools,
    verbose=True,
    return_intermediate_steps=True,
    handle_parsing_errors=(
        "FORMAT REMINDER: After 'Action:' you MUST output\n"
        "Action Input: <valid code with NO backticks>\n"
        "If you omitted it, continue by providing Action Input only."
    ),
)

# =============================
# ğŸ”„ RunnableWithMessageHistory
# =============================
agent_with_history = RunnableWithMessageHistory(
    agent,
    lambda session_id: history,
    input_messages_key="input",
    history_messages_key="chat_history",
)

# =============================
# ğŸ’» UI ë° ì‹¤í–‰ ë¡œì§
# =============================
st.write("---")
user_q = st.chat_input(
    "Ask about your data (ì˜ˆ: 'telemetry_value ì»¬ëŸ¼ì˜ ì´ìƒì ì„ ë°•ìŠ¤ í”Œë¡¯ìœ¼ë¡œ ê²€í† í•´ì¤˜', "
    "'describe_columnsë¡œ ì´ìƒì ì˜ ê²½ê³„ê°’ì„ ì•Œë ¤ì¤˜', "
    "'load_df_b() í›„ df_Aì™€ ì¡°ì¸í•´ì„œ ë¶„ì„í•´ì¤˜')"
)

if user_q:
    # ì±„íŒ… íˆìŠ¤í† ë¦¬ ê¸°ë¡
    history.add_user_message(user_q)

    left, right = st.columns([1, 1])
    with left:
        st.subheader("ì‹¤ì‹œê°„ ì‹¤í–‰ ë¡œê·¸")
        # StreamlitCallbackHandlerëŠ” ì¤‘ê°„ ë‹¨ê³„ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ë³´ì—¬ì¤ë‹ˆë‹¤.
        st_cb = StreamlitCallbackHandler(st.container())

    # ì‚¬ìš©ì ì •ì˜ ì½œë°± (íƒ€ì„ë¼ì¸ ë””ë²„ê¹…ìš©)
    collector = SimpleCollectCallback()

    with st.spinner("Thinking with Gemini..."):
        try:
            result = agent_with_history.invoke(
                {"input": user_q},
                {
                    # StdOutCallbackHandlerëŠ” í„°ë¯¸ë„ì— ë¡œê·¸ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.
                    "callbacks": [st_cb, collector, StdOutCallbackHandler()],
                    "configurable": {"session_id": "konlo_ssid"},
                }
            )
        except Exception as e:
            st.error(f"Agent ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            result = {"output": f"Agent ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"}

    st.success("Done.")

    final = result.get("output", "Agentê°€ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
    with right:
        st.subheader("Answer")
        # LLMì˜ ìµœì¢… ë‹µë³€ì„ ì±„íŒ…ì°½ì— í‘œì‹œ
        history.add_ai_message(final)
        st.write(final)

        # í”Œë¡¯ ë Œë”ë§
        figs = [plt.figure(n) for n in plt.get_fignums()]
        for f in figs:
            f.set_size_inches(6, 4)
            f.set_dpi(100)
            st.pyplot(f, use_container_width=True)
        plt.close("all")

    # intermediate steps (ë””ë²„ê¹… ì •ë³´)
    st.write("---")
    st.subheader("intermediate_steps (íˆ´ ì‹¤í–‰ ìƒì„¸)")
    steps = result.get("intermediate_steps", [])
    if steps:
        for i, (action, observation) in enumerate(steps, 1):
            with st.expander(f"Step {i}: {action.tool}"):
                st.markdown("**tool_input**")
                st.code(str(action.tool_input))
                st.markdown("**observation**")
                st.code(observation)
    else:
        st.info("intermediate_steps ë¹„ì–´ ìˆìŒ")

    # ì½œë°± íƒ€ì„ë¼ì¸ (ë””ë²„ê¹… ì •ë³´)
    st.write("---")
    st.subheader("ì½œë°± ì´ë²¤íŠ¸ íƒ€ì„ë¼ì¸ (Simple Collect)")
    if collector.events:
        for j, e in enumerate(collector.events, 1):
            with st.expander(f"Event {j}: {e.get('type')}"):
                if e.get("type") == "llm_start":
                    prompts = e.get("prompts", [])
                    for k, p in enumerate(prompts, 1):
                        st.markdown(f"**Prompt {k}**")
                        st.code(p)
                else:
                    st.write(e)
    else:
        st.info("ì½œë°± ì´ë²¤íŠ¸ ì •ë³´ ì—†ìŒ")

# =============================
# Streamlit ì±„íŒ… ê¸°ë¡ í‘œì‹œ
# =============================
st.sidebar.title("Chat History")
for msg in history.messages:
    if msg.type == "human":
        with st.sidebar:
            st.chat_message("user").write(msg.content)
    elif msg.type == "ai":
        # AI ë©”ì‹œì§€ëŠ” ìµœì¢… ë‹µë³€ì´ ì´ë¯¸ ë©”ì¸ì— í‘œì‹œë˜ì—ˆìœ¼ë¯€ë¡œ, ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ í‘œì‹œ
        with st.sidebar:
            st.chat_message("assistant").write(msg.content[:50] + "...")