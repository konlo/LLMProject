import io
import os
import zipfile
from typing import Any, Dict, List, Tuple

import duckdb
import matplotlib.pyplot as plt
import pandas as pd
import streamlit as st
from dotenv import load_dotenv
from langchain.agents import AgentExecutor, create_react_agent
from langchain.callbacks import StdOutCallbackHandler
from langchain_community.callbacks.streamlit import StreamlitCallbackHandler
from langchain_community.chat_message_histories import StreamlitChatMessageHistory
from langchain_core.callbacks.base import BaseCallbackHandler
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_core.tools import tool
from langchain_experimental.tools.python.tool import PythonAstREPLTool
from langchain_google_genai import ChatGoogleGenerativeAI


# =======================================================================
# Callback collection helper
# =======================================================================
class SimpleCollectCallback(BaseCallbackHandler):
    """Minimal callback handler to capture agent events for debugging."""

    def __init__(self) -> None:
        self.events: List[Dict[str, Any]] = []

    def on_tool_error(self, error: Exception, **kwargs: Any) -> Any:
        self.events.append({"type": "tool_error", "error": str(error)})

    def on_llm_start(
        self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any
    ) -> None:
        self.events.append({"type": "llm_start", "prompts": prompts})

    def on_llm_end(self, response: Any, **kwargs: Any) -> None:
        self.events.append({"type": "llm_end"})

    def on_agent_action(self, action: Any, **kwargs: Any) -> Any:  # pragma: no cover
        return None

    def on_agent_finish(self, finish: Any, **kwargs: Any) -> Any:  # pragma: no cover
        return None


# =======================================================================
# Streamlit configuration & defaults
# =======================================================================
load_dotenv()

PAGE_TITLE = "DF Chatbot (Gemini)"
PAGE_ICON = "âœ¨"
PAGE_LAYOUT = "wide"
CHAT_INPUT_PLACEHOLDER = (
    "Ask about your data (ì˜ˆ: 'telemetry_value ì»¬ëŸ¼ì˜ ì´ìƒì ì„ ë°•ìŠ¤ í”Œë¡¯ìœ¼ë¡œ ê²€í† í•´ì¤˜', "
    "'describe_columnsë¡œ ì´ìƒì ì˜ ê²½ê³„ê°’ì„ ì•Œë ¤ì¤˜', "
    "'load_df_b() í›„ df_Aì™€ ì¡°ì¸í•´ì„œ ë¶„ì„í•´ì¤˜')"
)
DEFAULT_DATA_DIR = os.getenv("DATA_DIR", "/Users/najongseong/dataset")
DEFAULT_DF_B_NAME = "telemetry_raw.csv"
DEFAULT_PRIMARY_PLACEHOLDER = "stormtrooper.csv"
SELECT_PLACEHOLDER = "--- Select a file ---"
CHAT_HISTORY_KEY = "lc_msgs:dfchat"

SESSION_DEFAULTS = {
    "DATA_DIR": lambda: DEFAULT_DATA_DIR,
    "df_A_data": lambda: None,
    "df_A_name": lambda: "No Data",
    "csv_path": lambda: os.path.join(DEFAULT_DATA_DIR, DEFAULT_PRIMARY_PLACEHOLDER),
}


def configure_page() -> None:
    st.set_page_config(page_title=PAGE_TITLE, page_icon=PAGE_ICON, layout=PAGE_LAYOUT)
    st.title("âœ¨ DataFrame Chatbot (Gemini + LangChain)")
    st.caption("Gemini + Python tool(ReAct)ë¡œ DataFrameì„ ë¶„ì„í•˜ê³  ì´ìƒì ì„ ê²€í† í•©ë‹ˆë‹¤.")


def init_session_state() -> None:
    for key, default in SESSION_DEFAULTS.items():
        if key not in st.session_state:
            st.session_state[key] = default() if callable(default) else default


def get_active_data_dir() -> str:
    return st.session_state.get("DATA_DIR", DEFAULT_DATA_DIR)


def load_primary_dataframe(path: str, display_name: str) -> Tuple[bool, str]:
    """Load df_A from disk and update session state."""
    try:
        dataframe = pd.read_csv(path)
    except Exception as exc:  # pylint: disable=broad-except
        st.session_state["df_A_data"] = None
        st.session_state["df_A_name"] = "Load Failed"
        st.session_state["csv_path"] = ""
        return False, f"CSV ë¡œë“œ ì‹¤íŒ¨: {path}\n{exc}"

    st.session_state["df_A_data"] = dataframe
    st.session_state["df_A_name"] = display_name
    st.session_state["csv_path"] = path
    return True, f"Loaded file: {display_name} (Shape: {dataframe.shape})"


def render_sidebar_controls() -> None:
    with st.sidebar:
        st.markdown("### ğŸ—‚ï¸ 1. ë°ì´í„° í´ë” ì„¤ì •")
        new_data_dir = st.text_input(
            "Enter CSV Directory Path",
            value=get_active_data_dir(),
            key="data_dir_input",
        )

        if st.button("Set Directory"):
            if os.path.isdir(new_data_dir):
                st.session_state["DATA_DIR"] = new_data_dir
                st.session_state["df_A_data"] = None
                st.session_state["df_A_name"] = "No Data"
                st.session_state["csv_path"] = ""
                st.success(f"Directory set to: `{new_data_dir}`")
                st.rerun()
            else:
                st.error(f"Invalid directory path: `{new_data_dir}`")

        current_dir = get_active_data_dir()
        st.markdown("---")
        st.markdown("### ğŸ“„ 2. df_A CSV íŒŒì¼ ì„ íƒ")
        st.caption(f"Search directory: `{current_dir}`")

        csv_files: List[str] = []
        try:
            if os.path.isdir(current_dir):
                csv_files = sorted(
                    f
                    for f in os.listdir(current_dir)
                    if f.lower().endswith(".csv")
                )
            else:
                st.warning("ìœ íš¨í•œ ë°ì´í„° ë””ë ‰í† ë¦¬ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
        except Exception as exc:  # pylint: disable=broad-except
            st.error(f"í´ë” ì ‘ê·¼ ì˜¤ë¥˜: {exc}")

        selected_file = st.selectbox(
            "Select CSV file for df_A",
            options=[SELECT_PLACEHOLDER] + csv_files,
            key="file_selector",
        )

        if st.button("Load Selected File"):
            if selected_file and selected_file != SELECT_PLACEHOLDER:
                file_path = os.path.join(current_dir, selected_file)
                success, message = load_primary_dataframe(file_path, selected_file)
                if success:
                    st.success(message)
                    st.rerun()
                else:
                    st.error(message)
            else:
                st.warning("íŒŒì¼ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")

        st.markdown("---")
        st.caption(
            f"**í˜„ì¬ ë¡œë“œ íŒŒì¼ ê²½ë¡œ:** `"
            f"{st.session_state.get('csv_path', 'Not loaded')}`"
        )
        st.caption(f"df_B ê¸°ë³¸ ê°€ì • íŒŒì¼: `{DEFAULT_DF_B_NAME}`")


def ensure_dataframe_loaded() -> Tuple[pd.DataFrame, str]:
    dataframe = st.session_state.get("df_A_data")
    display_name = st.session_state.get("df_A_name", "No Data")
    if dataframe is None:
        st.error("ë¶„ì„í•  DataFrame (df_A)ì„ ë¡œë“œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ìœ íš¨í•œ ë””ë ‰í† ë¦¬ì™€ CSV íŒŒì¼ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")
        st.stop()
    return dataframe, display_name


def fetch_api_key_or_stop() -> str:
    api_key = os.getenv("GOOGLE_API_KEY")
    if not api_key:
        st.error("GOOGLE_API_KEYê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. .env ë˜ëŠ” í™˜ê²½ë³€ìˆ˜ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        st.stop()
    return api_key


def create_llm(api_key: str) -> ChatGoogleGenerativeAI:
    return ChatGoogleGenerativeAI(
        model="gemini-2.5-flash",
        api_key=api_key,
        temperature=0,
        max_tokens=None,
        timeout=None,
        max_retries=2,
        streaming=True,
    )


def render_preview(dataframe: pd.DataFrame, display_name: str) -> None:
    st.subheader("Preview")
    st.write(f"**Loaded CSV for df_A:** `{display_name}` (Shape: {dataframe.shape})")
    st.dataframe(dataframe.head(10), use_container_width=True)


def build_toolkit(dataframe: pd.DataFrame) -> List[Any]:
    pytool = PythonAstREPLTool(
        globals={
            "pd": pd,
            "plt": plt,
            "df": dataframe,
            "df_A": dataframe,
            "df_B": None,
            "loading_df": None,
        },
        name="python_repl_ast",
        description=(
            "Execute Python on df_A/df_B with pandas/matplotlib. 'df' aliases df_A. "
            "Use this for custom computation, plotting (e.g., boxplot for outliers), and advanced data manipulation."
        ),
    )

    @tool
    def load_loading_csv(filename: str) -> str:
        """Load a CSV from DATA_DIR into 'loading_df'."""
        current_dir = get_active_data_dir()
        path = os.path.join(current_dir, filename)
        try:
            new_df = pd.read_csv(path)
        except Exception as exc:  # pylint: disable=broad-except
            return f"Failed to load {path}: {exc}"
        pytool.globals["loading_df"] = new_df
        preview = new_df.head(10).to_markdown(index=False)
        shape = f"{new_df.shape[0]} rows x {new_df.shape[1]} cols"
        return (
            f"Loaded {filename} from {current_dir} into loading_df\n"
            f"Shape: {shape}\n\nPreview (head):\n{preview}"
        )

    @tool
    def describe_columns(cols: str = "") -> str:
        """Describe selected columns (loading_df preferred, fallback to df_A)."""
        if pytool.globals.get("loading_df") is not None:
            current_df = pytool.globals["loading_df"]
            source = "loading_df"
        else:
            current_df = pytool.globals.get("df_A") or dataframe
            source = "df_A"

        use_cols = [c.strip() for c in cols.split(",") if c.strip()] or list(current_df.columns)
        missing = [c for c in use_cols if c not in current_df.columns]
        if missing:
            return f"Missing columns: {missing}\nAvailable: {list(current_df.columns)}"

        desc = current_df[use_cols].describe(include="all").transpose()
        shape = f"{current_df.shape[0]} rows x {current_df.shape[1]} cols"
        return f"[source={source} | shape={shape}]\n\n" + desc.to_markdown()

    @tool
    def save_plots_zip() -> str:
        """Zip all current matplotlib figures. Use after plotting with python_repl_ast."""
        figures = [plt.figure(num) for num in plt.get_fignums()]
        if not figures:
            return "No figures to save."
        buffer = io.BytesIO()
        with zipfile.ZipFile(buffer, "w", zipfile.ZIP_DEFLATED) as zf:
            for index, figure in enumerate(figures, start=1):
                img = io.BytesIO()
                figure.savefig(img, format="png", dpi=100, bbox_inches="tight")
                img.seek(0)
                zf.writestr(f"plot_{index}.png", img.read())
        return f"Zipped {len(figures)} plots. Bytes={len(buffer.getvalue())}"

    @tool
    def load_df_b(filename: str = "") -> str:
        """Load a CSV into 'df_B'. Defaults to telemetry_raw.csv under DATA_DIR."""
        current_dir = get_active_data_dir()
        if not filename:
            path = os.path.join(current_dir, DEFAULT_DF_B_NAME)
            show_name = os.path.basename(path)
        else:
            path = os.path.join(current_dir, filename) if not os.path.isabs(filename) else filename
            show_name = os.path.basename(path)
        try:
            new_df = pd.read_csv(path)
        except Exception as exc:  # pylint: disable=broad-except
            return f"Failed to load df_B from {path}: {exc}"
        pytool.globals["df_B"] = new_df
        preview = new_df.head(10).to_markdown(index=False)
        return (
            f"Loaded df_B from '{show_name}' (full: {path}) with shape {new_df.shape}\n\n"
            f"Preview:\n{preview}"
        )

    @tool
    def describe_columns_on(target: str = "A", cols: str = "") -> str:
        """Describe columns from df_A or df_B."""
        target_clean = (target or "A").strip().upper()
        if target_clean == "B":
            df_b = pytool.globals.get("df_B")
            if df_b is None:
                return "df_B is not loaded. Use load_df_b() first."
            current_df = df_b
            source = "df_B"
        else:
            current_df = pytool.globals.get("df_A") or dataframe
            source = "df_A"

        use_cols = [c.strip() for c in cols.split(",") if c.strip()] or list(current_df.columns)
        missing = [c for c in use_cols if c not in current_df.columns]
        if missing:
            return f"Missing columns: {missing}\nAvailable: {list(current_df.columns)}"

        desc = current_df[use_cols].describe(include="all").transpose()
        shape = f"{current_df.shape[0]} rows x {current_df.shape[1]} cols"
        return f"[source={source} | shape={shape}]\n\n" + desc.to_markdown()

    @tool
    def sql_on_dfs(query: str) -> str:
        """Run DuckDB SQL over df_A and (if loaded) df_B."""
        try:
            duckdb.register("df_A", pytool.globals.get("df_A") or dataframe)
            if pytool.globals.get("df_B") is not None:
                duckdb.register("df_B", pytool.globals["df_B"])
            output = duckdb.sql(query).df()
            return output.head(200).to_markdown(index=False)
        except Exception as exc:  # pylint: disable=broad-except
            return f"SQL error: {exc}"

    @tool
    def propose_join_keys() -> str:
        """Suggest join key candidates between df_A and df_B."""
        df_a = pytool.globals.get("df_A") or dataframe
        df_b = pytool.globals.get("df_B")
        if df_b is None:
            return "df_B is not loaded."

        def dtype_signature(series: pd.Series) -> str:
            dtype_str = str(series.dtype)
            if "int" in dtype_str:
                return "int"
            if "float" in dtype_str:
                return "float"
            if any(keyword in dtype_str for keyword in ("datetime", "date", "time")):
                return "datetime"
            return "str"

        pairs = [
            (column, dtype_signature(df_a[column]))
            for column in df_a.columns
            if column in df_b.columns and dtype_signature(df_a[column]) == dtype_signature(df_b[column])
        ]

        if not pairs:
            return "No obvious same-name & same-type keys. Consider mapping tables or casting."

        markdown_rows = "\n".join([f"| {key} | {dtype_} |" for key, dtype_ in pairs])
        return "Candidate join keys (same name & type):\n| key | dtype |\n|---|---|\n" + markdown_rows

    @tool
    def align_time_buckets(target: str = "A", column: str = "ts", freq: str = "H") -> str:
        """Resample time-like column to buckets and store as df_A_bucketed or df_B_bucketed."""
        target_clean = (target or "A").strip().upper()
        if target_clean == "B":
            current_df = pytool.globals.get("df_B")
            if current_df is None:
                return "df_B is not loaded."
            df_key = "df_B_bucketed"
        else:
            current_df = pytool.globals.get("df_A") or dataframe
            df_key = "df_A_bucketed"

        if column not in current_df.columns:
            return f"Column '{column}' not in df_{target_clean}. Available: {list(current_df.columns)}"

        bucketed = current_df.copy()
        bucketed[column] = pd.to_datetime(bucketed[column], errors="coerce")
        bucket_col = f"{column}_bucket"
        bucketed[bucket_col] = bucketed[column].dt.to_period(freq).dt.to_timestamp()
        pytool.globals[df_key] = bucketed
        preview = bucketed[[bucket_col]].head().to_markdown(index=False)
        return f"Created {df_key} with '{bucket_col}' at freq={freq}.\nPreview:\n{preview}"

    return [
        pytool,
        load_loading_csv,
        describe_columns,
        save_plots_zip,
        load_df_b,
        describe_columns_on,
        sql_on_dfs,
        propose_join_keys,
        align_time_buckets,
    ]


def build_react_prompt(dataframe: pd.DataFrame) -> ChatPromptTemplate:
    df_head_txt = dataframe.head().to_string(index=False)
    return ChatPromptTemplate.from_messages([
        (
            "system",
            "You are working with pandas dataframes. The main dataframe is `df_A` (alias: df). "
            "You may also load a related dataframe as `df_B` for root-cause analysis.\n\n"
            "Tools available:\n{tools}\nUse only tool names from: {tool_names}.\n"
            "If you use sql_on_dfs, available tables are df_A and (if loaded) df_B.\n\n"
            "Tool routing guide:\n"
            "- schema/summary/initial outlier bounds â†’ describe_columns or describe_columns_on\n"
            "- load file â†’ load_loading_csv or load_df_b\n"
            "- SQL/join/aggregation â†’ sql_on_dfs\n"
            "- **custom compute/plots/outlier analysis** â†’ **python_repl_ast**\n"
            "- suggest join keys â†’ propose_join_keys\n"
            "- align timestamps to buckets â†’ align_time_buckets\n\n"
            "**âš ï¸ ì´ìƒì (Outlier) ê²€í†  ì§€ì¹¨:**\n"
            "1. **describe_columns**ë¥¼ ì‚¬ìš©í•˜ì—¬ ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ì˜ min/max/std/ì‚¬ë¶„ìœ„ìˆ˜(Q1, Q3)ë¥¼ í™•ì¸í•˜ì„¸ìš”.\n"
            "2. **python_repl_ast**ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°•ìŠ¤ í”Œë¡¯(Box Plot)ì„ ê·¸ë ¤ ì‹œê°ì ìœ¼ë¡œ ì´ìƒì ì„ í™•ì¸í•˜ì„¸ìš”.\n"
            "   - ì˜ˆì‹œ: `df['value_col'].plot(kind='box'); plt.title('Box Plot'); plt.show()`\n"
            "3. **python_repl_ast**ë¥¼ ì‚¬ìš©í•˜ì—¬ IQR(Q3 + 1.5*IQR) ë“±ì˜ í†µê³„ì  ê²½ê³„ ì¡°ê±´ì„ ê³„ì‚°í•˜ê³ , ì´ìƒì ì„ í•„í„°ë§í•˜ì—¬ í™•ì¸í•˜ì„¸ìš”.\n\n"
            "**ğŸš¨ Python ì‹¤í–‰ í•„ìˆ˜ ì§€ì¹¨ (ë³€ìˆ˜ ì§€ì†ì„± ì˜¤ë¥˜ ë°©ì§€):**\n"
            "1. ë³µì¡í•œ ì‘ì—…(ë°ì´í„° ì¤€ë¹„, ëª¨ë¸ ì •ì˜, í•™ìŠµ, ì˜ˆì¸¡ ë“±)ì€ ë³€ìˆ˜ ì§€ì†ì„±ì„ ë³´ì¥í•˜ê¸° ìœ„í•´ **ë°˜ë“œì‹œ í•˜ë‚˜ì˜ `python_repl_ast` íˆ´ í˜¸ì¶œ ë‚´**ì—ì„œ ì‹¤í–‰í•˜ì‹­ì‹œì˜¤.\n"
            "2. ì½”ë“œë¥¼ ì—¬ëŸ¬ 'Action:'ìœ¼ë¡œ ë¶„í• í•˜ì§€ ë§ˆì‹­ì‹œì˜¤. ë³€ìˆ˜(`model`, `X_train`, `results` ë“±)ê°€ ë‹¤ìŒ ë‹¨ê³„ì—ì„œ ìœ ì‹¤ë˜ì–´ ì˜¤ë¥˜ê°€ ë°œìƒí•©ë‹ˆë‹¤.\n"
            "3. ìµœì¢… ê²°ê³¼ë¥¼ ì‚¬ìš©ìì—ê²Œ ë³´ì—¬ì¤„ ë•ŒëŠ” ì½”ë“œ ë¸”ë¡ì˜ ë§ˆì§€ë§‰ì— `print(ê²°ê³¼ë³€ìˆ˜)` ëª…ë ¹ì„ í¬í•¨í•˜ì—¬ **Observation**ìœ¼ë¡œ ê²°ê³¼ë¥¼ ëª…í™•íˆ ë°˜í™˜í•´ì•¼ í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ìµœì¢…ì ìœ¼ë¡œ í™•ì¸í•˜ëŠ” ê²½ìš° `print(results)` ë˜ëŠ” `print(accuracy)`ë¥¼ ì‚¬ìš©í•˜ì‹­ì‹œì˜¤.\n\n"
            "Parsing recovery:\n"
            "- If you output 'Action:' without 'Action Input:', immediately continue with only 'Action Input: <...>'.\n"
            "- Do not wrap code in backticks.\n"
            "- Keep Action Input minimal, valid, and executable.\n\n"
            """
            "   **ì‹¤í–‰ ê·œì¹™:**\n"
            "   1. ê° ë„êµ¬ëŠ” ìµœëŒ€ 3ë²ˆê¹Œì§€ë§Œ ì‹œë„\n"
            "   2. ë™ì¼í•œ ì˜¤ë¥˜ê°€ 2ë²ˆ ë°œìƒí•˜ë©´ ì¦‰ì‹œ ë‹¤ë¥¸ ë°©ë²• ì‹œë„\n"
            "   3. ë¬´í•œ ë£¨í”„ ê°ì§€ ì‹œ ìë™ìœ¼ë¡œ ì¤‘ë‹¨\n\n"
            "   **ì˜¤ë¥˜ë³„ ëŒ€ì²˜ë²•:**\n"
            "   - TypeError (sort_values): â†’ np.argsort() ë˜ëŠ” sorted() ì‚¬ìš©\n"
            "   - KeyError: â†’ ì»¬ëŸ¼ëª… í™•ì¸ í›„ ì¬ì‹œë„\n"
            "   - AttributeError: â†’ ê°ì²´ íƒ€ì… í™•ì¸ í›„ ì ì ˆí•œ ë©”ì„œë“œ ì‚¬ìš©\n\n"
            "   **ìƒíƒœ ì²´í¬í¬ì¸íŠ¸:**\n"
            "   ë§¤ 3ë²ˆì§¸ ì•¡ì…˜ë§ˆë‹¤:\n"
            "   - ì§„í–‰ ìƒí™© ìš”ì•½\n"
            "   - ëª©í‘œ ë‹¬ì„±ë„ í™•ì¸\n"
            "   - í•„ìš”ì‹œ ì „ëµ ìˆ˜ì •\n"
            """
            "ALWAYS follow this exact format:\n"
            "Question: <restated question>\n"
            "Thought: <brief reasoning>\n"
            "Action: <ONE tool name from {tool_names}>\n"
            "Action Input: <valid code with NO backticks>\n"
            "Observation: <tool result>\n"
            "(Repeat Thought/Action/Action Input/Observation as needed)\n"
            "Thought: I now know the final answer\n"
            "Final Answer: <answer>\n\n"
            f"This is the result of print(df_A.head()):\n{df_head_txt}\n",
        ),
        MessagesPlaceholder("chat_history", optional=True),
        ("human", "{input}"),
        ("assistant", "{agent_scratchpad}"),
    ])


def create_agent_executor(
    llm: ChatGoogleGenerativeAI, tools: List[Any], prompt: ChatPromptTemplate
) -> AgentExecutor:
    react_runnable = create_react_agent(llm, tools, prompt=prompt)
    return AgentExecutor(
        agent=react_runnable,
        tools=tools,
        verbose=True,
        return_intermediate_steps=True,
        handle_parsing_errors=(
            "FORMAT REMINDER: After 'Action:' you MUST output\n"
            "Action Input: <valid code with NO backticks>\n"
            "If you omitted it, continue by providing Action Input only."
        ),
    )


def build_agent_with_history(agent: AgentExecutor, history: StreamlitChatMessageHistory) -> RunnableWithMessageHistory:
    return RunnableWithMessageHistory(
        agent,
        lambda session_id: history,
        input_messages_key="input",
        history_messages_key="chat_history",
    )


def handle_user_query(agent_with_history: RunnableWithMessageHistory) -> None:
    st.write("---")
    user_question = st.chat_input(CHAT_INPUT_PLACEHOLDER)
    if not user_question:
        return

    left, right = st.columns([1, 1])
    with left:
        st.subheader("ì‹¤ì‹œê°„ ì‹¤í–‰ ë¡œê·¸")
        streamlit_callback = StreamlitCallbackHandler(st.container())

    collector = SimpleCollectCallback()

    with st.spinner("Thinking with Gemini..."):
        try:
            result = agent_with_history.invoke(
                {"input": user_question},
                {
                    "callbacks": [streamlit_callback, collector, StdOutCallbackHandler()],
                    "configurable": {"session_id": "konlo_ssid"},
                },
            )
        except Exception as exc:  # pylint: disable=broad-except
            st.error(f"Agent ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {exc}")
            result = {"output": f"Agent ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {exc}"}

    st.success("Done.")

    final_output = result.get("output", "Agentê°€ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
    with right:
        st.subheader("Answer")
        st.write(final_output)

        figures = [plt.figure(num) for num in plt.get_fignums()]
        for figure in figures:
            figure.set_size_inches(6, 4)
            figure.set_dpi(100)
            st.pyplot(figure, use_container_width=True)
        plt.close("all")

    st.write("---")
    st.subheader("intermediate_steps (íˆ´ ì‹¤í–‰ ìƒì„¸)")
    steps = result.get("intermediate_steps", [])
    if steps:
        for index, (action, observation) in enumerate(steps, start=1):
            with st.expander(f"Step {index}: {action.tool}"):
                st.markdown("**tool_input**")
                st.code(str(action.tool_input))
                st.markdown("**observation**")
                st.code(observation)
    else:
        st.info("intermediate_steps ë¹„ì–´ ìˆìŒ")

    st.write("---")
    st.subheader("ì½œë°± ì´ë²¤íŠ¸ íƒ€ì„ë¼ì¸ (Simple Collect)")
    if collector.events:
        for index, event in enumerate(collector.events, start=1):
            with st.expander(f"Event {index}: {event.get('type')}"):
                if event.get("type") == "llm_start":
                    for prompt_index, prompt in enumerate(event.get("prompts", []), start=1):
                        st.markdown(f"**Prompt {prompt_index}**")
                        st.code(prompt)
                else:
                    st.write(event)
    else:
        st.info("ì½œë°± ì´ë²¤íŠ¸ ì •ë³´ ì—†ìŒ")


def render_history_sidebar(history: StreamlitChatMessageHistory) -> None:
    st.sidebar.title("Chat History")
    for message in history.messages:
        if message.type == "human":
            st.sidebar.chat_message("user").write(message.content)
        elif message.type == "ai":
            st.sidebar.chat_message("assistant").write(message.content[:50] + "...")


def main() -> None:
    configure_page()
    init_session_state()
    render_sidebar_controls()

    dataframe, display_name = ensure_dataframe_loaded()
    api_key = fetch_api_key_or_stop()
    llm = create_llm(api_key)

    render_preview(dataframe, display_name)

    history = StreamlitChatMessageHistory(key=CHAT_HISTORY_KEY)
    tools = build_toolkit(dataframe)
    prompt = build_react_prompt(dataframe)
    agent = create_agent_executor(llm, tools, prompt)
    agent_with_history = build_agent_with_history(agent, history)

    handle_user_query(agent_with_history)
    render_history_sidebar(history)


main()
